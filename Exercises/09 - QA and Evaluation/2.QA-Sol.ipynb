{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cba48da",
   "metadata": {},
   "source": [
    "\n",
    "## üß© QA (Extractive, Generative) and RAG\n",
    "\n",
    "In this notebook, we‚Äôll explore **Question Answering (QA)** and **Retrieval-Augmented Generation (RAG)** ‚Äî two key paradigms for connecting large language models with factual information.\n",
    "\n",
    "We‚Äôll begin with **Extractive QA**, where a model identifies an answer span directly from a passage using *sequence labeling* with a **BERT-style encoder**.  \n",
    "Then, we‚Äôll move to **Generative QA**, where models like **BART** or **GPT** produce free-form answers in natural language, demonstrating greater flexibility but also higher risk of **hallucination**.\n",
    "\n",
    "Next, we‚Äôll discuss the **limitations** of both approaches ‚Äî extractive QA can be rigid and context-limited, while generative QA may generate fluent but incorrect answers.  \n",
    "To address these issues, we‚Äôll introduce **Retrieval-Augmented Generation (RAG)**, which enriches the model‚Äôs context with relevant external knowledge to improve **factuality** and **reduce hallucinations**.\n",
    "\n",
    "\n",
    "The goal of this notebook is **not just to run QA models**, but to **understand their design trade-offs** and how retrieval-based methods can make generation more trustworthy and grounded in evidence.  \n",
    "\n",
    "By the end of this notebook, you‚Äôll have a clear understanding of:\n",
    "- How extractive and generative QA differ in architecture and behavior,  \n",
    "- Why hallucination occurs in generative systems, and  \n",
    "- How RAG mitigates these issues by integrating retrieval with generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c488ba",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Stanford Question Answering Dataset (**SQuAD**) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "SQuAD 1.1 contains 100,000+ question-answer pairs on 500+ articles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e00d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "\n",
      "Example 2\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: What is in front of the Notre Dame Main Building?\n",
      "Answer: {'text': ['a copper statue of Christ'], 'answer_start': [188]}\n",
      "\n",
      "Example 3\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden...\n",
      "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "Answer: {'text': ['the Main Building'], 'answer_start': [279]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# for resource constraints, we only use a subset of the training set, if you have enough resources, feel free to use the full dataset\n",
    "squad_sample = dataset[\"train\"].select(range(10))\n",
    "\n",
    "#print first 3 examples\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(f\"Context: {squad_sample[i]['context'][:100]}...\")\n",
    "    print(f\"Question: {squad_sample[i]['question']}\")\n",
    "    print(f\"Answer: {squad_sample[i]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a4d65",
   "metadata": {},
   "source": [
    "### Extractive QA Model\n",
    "\n",
    "The class **`AutoModelForQuestionAnswering`** is part of Hugging Face‚Äôs `transformers` library and is specifically designed for **extractive question answering** tasks ‚Äî where the model identifies an answer **span** within a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad35358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who were special guests for the Super Bowl halftime show?\n",
      "Ground Truth Answer: Beyonc√© and Bruno Mars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "#load dataset and pick a case\n",
    "dataset = load_dataset(\"squad\")\n",
    "squad_sample = dataset[\"validation\"].select(range(1000))\n",
    "example = squad_sample[100]\n",
    "context = example[\"context\"]\n",
    "question = example[\"question\"]\n",
    "true_answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth Answer: {true_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9385004",
   "metadata": {},
   "source": [
    "`deepset/bert-base-cased-squad2` is a BERT base cased example model trained on SQuAD v2. You can also try different models. The model is based on bert model and designed for Extractive QA\n",
    "\n",
    "For more model details, please refer to this link :https://huggingface.co/deepset/bert-base-cased-squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d529a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and BERT-style QA model\n",
    "model_name = \"deepset/bert-base-cased-squad2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6d3fe",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "1. Feed the inputs into the model to obtain logits that represent how likely each token is the start or end of the answer span.\n",
    "`torch.no_grad()` is used to disable gradient computation during inference.\n",
    "\n",
    "2. Select the tokens with the highest start and end probabilities using `argmax`.\n",
    "3. Extract the predicted answer span from the input IDs and convert it back into natural language using the tokenizer. You can use `tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(...))`\n",
    "4. Display the model‚Äôs predicted answer, the true answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "577db8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: Beyonc√© and Bruno Mars\n",
      "Ground Truth Answer: Beyonc√© and Bruno Mars\n",
      "Start index: 63, End index: 67\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    ")\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")\n",
    "print (f\"Ground Truth Answer: {true_answer}\")\n",
    "print(f\"Start index: {start_idx.item()}, End index: {end_idx.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb60bb",
   "metadata": {},
   "source": [
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "Look at the **Example**\n",
    "\n",
    "`Example`:\n",
    "\n",
    "- **Context** Marie Curie was awarded the Nobel Prize in Physics in 1903.\n",
    "\n",
    "- **Question** Did Marie Curie win a Nobel Prize?\n",
    "\n",
    "- **Answer** Yes\n",
    "\n",
    "\n",
    "\n",
    "**Discuss**:  can Extractive QA models still answer correctly? Why or why not?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57696b4",
   "metadata": {},
   "source": [
    "No, They can only extract spans that already exist in the passage, so they cannot generate paraphrased or synthesized answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4c08b",
   "metadata": {},
   "source": [
    "### Generative QA Model\n",
    "\n",
    "The class **`AutoModelForCausalLM`** (Causal Language Modeling) is part of Hugging Face‚Äôs `transformers` library and is designed for **autoregressive text generation** ‚Äî where each new token is generated **based on all previously generated tokens**.  \n",
    "It is typically used with **decoder-only architectures** such as **GPT-2**, **GPT-Neo**, or **LLaMA**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee9b01",
   "metadata": {},
   "source": [
    "\n",
    "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on a dataset of 8 million web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fb3ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "example = squad_sample[100]\n",
    "context = example[\"context\"]\n",
    "question = example[\"question\"]\n",
    "true_answer = example[\"answers\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7e470",
   "metadata": {},
   "source": [
    "You can make a prompt like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79a5619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Answer the question based only on the given context.\\n\\n\"\n",
    "    f\"Context: {context}\\n\"\n",
    "    f\"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c1199",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----\n",
    "**`TODO:`**\n",
    "\n",
    "1. **Use the model‚Äôs `generate()` method**  \n",
    "   - Call `model.generate()` to produce new tokens based on your input prompt.  \n",
    "   - Experiment with parameters such as:  \n",
    "     - `max_new_tokens` ‚Üí controls the maximum length of the generated output.  \n",
    "     - `do_sample`, `top_p`, and `temperature` ‚Üí adjust randomness and creativity in generation.  \n",
    "     - `eos_token_id` ‚Üí defines where the model should stop generating (end-of-sequence token).  \n",
    "\n",
    "\n",
    "2. **Convert the model‚Äôs token IDs back into readable text**  \n",
    "   - Use `tokenizer.decode(outputs[0], skip_special_tokens=True)` to transform the generated token IDs into plain text.  \n",
    "   - This step turns the model‚Äôs internal numerical predictions into human-readable language.\n",
    "\n",
    "3. **Extract only the model‚Äôs answer**  \n",
    "   - Remove the original prompt from the decoded text so that you keep just the generated response.  \n",
    "   - This helps you focus on what the model actually ‚Äúanswered,‚Äù rather than the repeated input.\n",
    "\n",
    "4. **Print and compare results**  \n",
    "\n",
    "\n",
    "For more details on how to use `generate()` and `decode()`, refer back to **Exercise 08 ‚Äì Generation**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62a34db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: The song \"I Know You Love Me\" was performed by the Los Angeles Kings and played in a special capacity at the Super Bowl. It was the third-\n",
      "Ground Truth Answer: Beyonc√© and Bruno Mars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=32,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "answer = generated[len(prompt):].strip()\n",
    "\n",
    "print(f\"Predicted Answer: {answer}\")\n",
    "print (f\"Ground Truth Answer: {true_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc9c22",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "**`TODO`**: **Discussion** Run the code multipletimes, does the model‚Äôs generated answer contain any hallucination ‚Äî information that was not stated or implied in the given context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668157f",
   "metadata": {},
   "source": [
    "Generative QA models often produce fluent but hallucinated answers, adding information that is not supported or mentioned in the provided context\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817207b0",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses.\n",
    "\n",
    "RAG helps reduce AI hallucinations by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\n",
    "\n",
    "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs.\n",
    "\n",
    "Please refer to Patrick's paper for more details: https://arxiv.org/abs/2005.11401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b610c",
   "metadata": {},
   "source": [
    "**Retrieval-Augmented Generation (RAG)** combines two key components ‚Äî an **information retriever** and a **text generator** ‚Äî to produce more factual and grounded answers.  \n",
    "\n",
    "Here‚Äôs the standard workflow:\n",
    "\n",
    "1. **Indexing / Document Preparation**  \n",
    "   - Build or load a *knowledge corpus* (e.g., Wikipedia articles, research papers, company documents).  \n",
    "   - Preprocess and store it in a searchable format (using BM25, dense embeddings, or a vector database).\n",
    "\n",
    "2. **Retrieval**  \n",
    "   - When a user asks a question, the retriever finds the top-k most relevant documents from the corpus.  \n",
    "   - Methods can be lexical (**BM25**) or semantic (**embedding-based models** like Sentence-BERT).\n",
    "\n",
    "3. **Context Construction**  \n",
    "   - Combine the retrieved passages into a single context block.  \n",
    "   - Optionally truncate or rank passages based on relevance or confidence scores.\n",
    "\n",
    "4. **Generation**  \n",
    "   - Pass the question + retrieved context to a **generative model**.  \n",
    "   - The model generates an answer *conditioned on both the query and the evidence*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254d96d",
   "metadata": {},
   "source": [
    "We will continue to use Groq for this task. If you‚Äôre not familiar with it, please refer to **Exercise 07 ‚Äì Post-training**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190673c",
   "metadata": {},
   "source": [
    "Let us try without RAG first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d09a8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff on December 1, 2023, the most recent score between Barcelona and Real Madrid was:\n",
      "\n",
      "Barcelona 2, Real Madrid 1 \n",
      "\n",
      "This match took place on October 29, 2023, in La Liga. However, please note that my information may not be up to date, and I recommend checking with a reliable sports source for the latest scores and updates.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The most recent score between Barcelona and Real Madrid.\",\n",
    "        }\n",
    "    ],\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eafb16",
   "metadata": {},
   "source": [
    "The answer is incorrect because the large language model **does not have access to the most up-to-date knowledge**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548ec1",
   "metadata": {},
   "source": [
    "### 1. build the index\n",
    "\n",
    "Building an index usually involves preprocessing a corpus (cleaning, tokenizing, or embedding the documents) and then storing them in a searchable structure such as a BM25 index or a vector database for fast retrieval during queries. This is a very small demo dataset ‚Äî the data comes from daily news articles collected from Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "882a292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Real Madrid 2-1 Barcelona (Oct 26, 2025) Game Analysis\",\n",
    "    \"Bill Gates calls for climate fight to shift focus\",\n",
    "    \"Fawlty Towers episode to air on BBC One in tribute to the late Prunella Scales\",\n",
    "    \"Climate Change Falls Over 20% Behind Top Global Concern in 2025\",\n",
    "    \"53.5 of EU services exports by large enterprises\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0b21d",
   "metadata": {},
   "source": [
    "### 2. Retrieval\n",
    "\n",
    "Here, we try to retrieve by `BM25`, BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. \n",
    "\n",
    "For more information, refer to https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fe262",
   "metadata": {},
   "source": [
    "You should install BM25 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31423435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in /Users/au805652/miniconda3/envs/exam/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /Users/au805652/miniconda3/envs/exam/lib/python3.10/site-packages (from rank_bm25) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9db9d7",
   "metadata": {},
   "source": [
    "Here is a Retrieval Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72defc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Real Madrid 2-1 Barcelona (Oct 26, 2025) Game Analysis\n",
      "BM25 Score: 3.9074354596958543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())\n",
    "\n",
    "\n",
    "tokenized_corpus = [tokenize(d) for d in docs]\n",
    "\n",
    "# build the index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# retrieval function: return top-k documents and their BM25 scores\n",
    "def retrieve_bm25(query, k=1):\n",
    "    q_tokens = tokenize(query)\n",
    "    scores = bm25.get_scores(q_tokens)             \n",
    "    topk_idx = sorted(range(len(scores)), key=lambda i: -scores[i])[:k]\n",
    "    return [(docs[i], float(scores[i])) for i in topk_idx]\n",
    "\n",
    "\n",
    "query = \"What is the score of the latest football match between Real Madrid and Barcelona?\"\n",
    "results = retrieve_bm25(query, k=1)\n",
    "for doc, score in results:\n",
    "    print(f\"Document: {doc}\\nBM25 Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30274e",
   "metadata": {},
   "source": [
    "### 3. Context Construction\n",
    "\n",
    "`TODO`: You should write a prompt that includes not only the question but also the context information. If you‚Äôre not familiar with writing prompts, please refer to **Exercise 07 ‚Äì Post-training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60be45d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Answer the question using the information provided in the context. \n",
      "Context:\n",
      "Real Madrid 2-1 Barcelona (Oct 26, 2025) Game Analysis\n",
      "\n",
      "Question:\n",
      "What is the score of the latest football match between Real Madrid and Barcelona?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = (\n",
    "    \"You are a helpful assistant. \"\n",
    "    \"Answer the question using the information provided in the context. \\n\"\n",
    "    f\"Context:\\n{results[0][0]}\\n\\n\"\n",
    "    f\"Question:\\n{query}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64982585",
   "metadata": {},
   "source": [
    "### 4. **Generation**  \n",
    "   \n",
    "Pass the question + retrieved context to a **generative model**.  \n",
    "\n",
    "`ToDo`: Construct a new request using our custom prompt\n",
    "Goal: Combine the retrieved context from BM25 and the query into a single prompt,\n",
    "then send it to the model for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40eb83e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the latest football match between Real Madrid and Barcelona is Real Madrid 2, Barcelona 1.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp25-exam",
   "language": "python",
   "name": "exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
