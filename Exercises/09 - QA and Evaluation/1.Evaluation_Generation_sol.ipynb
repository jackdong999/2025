{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456dfd6a",
   "metadata": {},
   "source": [
    "## üß© Evaluation of Text Generation\n",
    "\n",
    "In this notebook, we'll continue **look at text generation**. And this time, we will focus on **Evaluation**.\n",
    "\n",
    "Evaluation of Text Generation is a central challenge in natural language generation research. How we evaluate systems shapes not only model comparison but also our very definition of what counts as a ‚Äúgood‚Äù output. In this session, you will explore different types of evaluation metrics to uncover their strengths, limitations, and inherent biases.\n",
    "\n",
    "The goal of this notebook is **not to chase perfect evaluation scores**, but to **experiment** and **build intuition** about how evaluation of text generation actually works.  \n",
    "\n",
    "You‚Äôre encouraged to:  \n",
    "- Try out different evaluation metrics from various classes,  \n",
    "- Compare how they rate the same model outputs, and  \n",
    "- Reflect on when and why these metrics agree‚Äîor fail to.  \n",
    "\n",
    "In this notebook, we‚Äôll focus on two main types of evaluation metrics:  \n",
    "(a) **Content-overlap metrics**   \n",
    "(b) **Model-based metrics**.  \n",
    "\n",
    "Beyond these automatic methods, you‚Äôre also encouraged to **manually evaluate** some generated outputs yourself ‚Äî observe which metrics best align with your own intuition about quality.\n",
    "\n",
    "By the end of this notebook, you‚Äôll have a practical understanding of how to **evaluate text generation models** and a clearer sense of **what good evaluation really means**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb0d97",
   "metadata": {},
   "source": [
    "### üßÆ The `evaluate` Library\n",
    "\n",
    "[`evaluate`](https://huggingface.co/docs/evaluate) is a lightweight library from Hugging Face that provides a unified interface for computing a wide range of NLP evaluation metrics ‚Äî from classic ones like **BLEU**, and **Perplexity**, to modern model-based metrics such as **BERTScore** and **COMET**. \n",
    "\n",
    " Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. \n",
    "\n",
    "You can check more Metrics here: https://huggingface.co/evaluate-metric/spaces\n",
    "\n",
    " Each metric is a separate Python module, but for using any of them, there is a single entry point: `evaluate.load()`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3e8db",
   "metadata": {},
   "source": [
    "### üìè Content-overlap Metrics\n",
    "\n",
    "**Content-overlap metrics** evaluate how closely a generated text matches one or more reference texts by comparing their surface forms ‚Äî typically through word or n-gram overlap.  \n",
    "\n",
    "These metrics are simple, interpretable, and fast to compute, but they often fail to capture deeper semantic meaning or paraphrasing.\n",
    "\n",
    "In this notebook, we‚Äôll focus on two of the most widely used metrics in this category:  \n",
    "\n",
    "- **BLEU** ‚Äî computes the overlap of n-grams between generated and reference texts, and is widely used in **machine translation** and **summarization**.  \n",
    "  \n",
    "- **ROUGE** ‚Äî measures the overlap of n-grams, words, or word sequences, but is especially designed for **summarization** tasks, focusing on recall rather than precision.n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a162333",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine‚Äôs output and that of a human: ‚Äúthe closer a machine translation is to a professional human translation, the better it is‚Äù ‚Äì this is the central idea behind BLEU. \n",
    "\n",
    "BLEU and BLEU-derived metrics are most often used for machine translation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b7b31",
   "metadata": {},
   "source": [
    "You should first run `pip install evaluate` to install it. Then, You Can load evaluate by this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e510349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.94kB [00:00, 2.82MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 3.88MB/s]                   \n",
      "Downloading extra modules: 3.34kB [00:00, 2.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde4ed7",
   "metadata": {},
   "source": [
    "Here is an example texts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11583e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat sitting on the carpet.\"\n",
    "]\n",
    "references = [\n",
    "    [\"The cat sits on the mat.\"],\n",
    "    [\"A cat is on the carpet.\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64191865",
   "metadata": {},
   "source": [
    "------------\n",
    "**`TODO:`** Compute the BLEU score for the `predictions` list against the `references` list using the `bleu.compute()` function.  \n",
    "- Use the `predictions` variable as the input for the `predictions` parameter.  \n",
    "- Use the `references` variable as the input for the `references` parameter.  \n",
    "- Store the result in a variable named `results`.  \n",
    "- Print the BLEU score from the `results` dictionary using the key `'bleu'`.  \n",
    "This will help you understand how BLEU evaluates the overlap between the generated and reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93859d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.3976\n"
     ]
    }
   ],
   "source": [
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU score: {results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2f53f",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "BLEU also has several limitations, which we‚Äôll illustrate through examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20aeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [[\"The cat is on the mat.\"]]\n",
    "predictions_good = [\"A cat sits on the rug.\"]    \n",
    "predictions_bad = [\"The cat is not on the mat.\"]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb56515",
   "metadata": {},
   "source": [
    "Compute the scores of `predictions_good` and `predictions_bad` against the `references`, and **discuss** why this happens ‚Äî what limitation of BLEU does it reveal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9ff377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantically similar sentence BLEU: 0.0000\n",
      "Semantically wrong sentence BLEU:  0.5000\n"
     ]
    }
   ],
   "source": [
    "score_good = bleu.compute(predictions=predictions_good, references=references)\n",
    "score_bad = bleu.compute(predictions=predictions_bad, references=references)\n",
    "\n",
    "print(f\"Semantically similar sentence BLEU: {score_good['bleu']:.4f}\")\n",
    "print(f\"Semantically wrong sentence BLEU:  {score_bad['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b43f8",
   "metadata": {},
   "source": [
    "This happens because BLEU only measures surface-level n-gram overlap, without understanding the underlying semantics of the sentence. It rewards lexical similarity rather than meaning, revealing one of BLEU‚Äôs main limitations: it cannot distinguish between correct and incorrect meanings if the wording is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981aa1e",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.\n",
    "\n",
    "\n",
    "\n",
    "This metrics is a wrapper around Google Research reimplementation of ROUGE: https://github.com/google-research/google-research/tree/master/rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de364f",
   "metadata": {},
   "source": [
    "Unlike BLEU, which focuses on precision, ROUGE emphasizes **recall** ‚Äî how much of the reference text‚Äôs content is captured in the generated text.\n",
    "\n",
    "Here are the main variants you‚Äôll encounter:\n",
    "\n",
    "- **ROUGE-1** ‚Äî Measures the overlap of individual words (unigrams) between the prediction and reference.  \n",
    "  ‚Üí Captures basic lexical similarity.\n",
    "\n",
    "- **ROUGE-2** ‚Äî Measures the overlap of 2-word sequences (bigrams).  \n",
    "  ‚Üí Reflects fluency and short-phrase consistency.\n",
    "\n",
    "- **ROUGE-L** ‚Äî Based on the *Longest Common Subsequence (LCS)* between prediction and reference.  \n",
    "  ‚Üí Captures sentence-level structure and word order similarity.\n",
    "\n",
    "- **ROUGE-Lsum** ‚Äî A summary-level variant of ROUGE-L that averages the LCS-based recall across multiple sentences in the generated summary.  \n",
    "  ‚Üí More suitable for multi-sentence summarization tasks.\n",
    "\n",
    "üí° **Interpretation:**  \n",
    "Higher ROUGE scores generally indicate better content overlap with the reference, but like BLEU, ROUGE is still surface-based and does not measure semantic correctness or factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471262e8",
   "metadata": {},
   "source": [
    "Here is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a37dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"The cat is on the mat.\"\n",
    "\n",
    "prediction_normal = \"The cat is on the mat.\"\n",
    "prediction_paraphrase = \"A cat sits on the rug.\"\n",
    "prediction_negated = \"The cat is not on the mat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce93f1",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "**`TODO:`** Use the example above to compute **ROUGE** scores for your generated outputs.\n",
    "\n",
    "\n",
    "\n",
    "1. **Load the ROUGE metric** using `evaluate.load(\"rouge\")`.\n",
    "\n",
    "2. **Compute ROUGE** for your different predictions (e.g., `prediction_normal`, `prediction_paraphrase`, and `prediction_negated`) against the same reference.\n",
    "\n",
    "3. **Access specific ROUGE variants** such as ROUGE-1, ROUGE-2, ROUGE-L, or ROUGE-Lsum by indexing the result dictionary (e.g., `rouge.compute(...)[‚Äòrouge1‚Äô]`).\n",
    "\n",
    "4. **Compare the results** across the three predictions:  \n",
    "   - How do the scores differ between exact matches, paraphrases, and negated sentences?  \n",
    "   - Do higher ROUGE scores always correspond to better or more semantically accurate outputs?\n",
    "\n",
    "5. **Discuss your findings:**  \n",
    "   Consider where ROUGE may fail to capture semantic equivalence or meaning preservation.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b64c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Normal (near-identical wording)]\n",
      "ROUGE-1:  1.0000\n",
      "ROUGE-2:  1.0000\n",
      "ROUGE-L:  1.0000\n",
      "ROUGE-Lsum: 1.0000\n",
      "\n",
      "[Failure: paraphrase (semantic match, low surface overlap)]\n",
      "ROUGE-1:  0.5000\n",
      "ROUGE-2:  0.2000\n",
      "ROUGE-L:  0.5000\n",
      "ROUGE-Lsum: 0.5000\n",
      "\n",
      "[Failure: negation (surface match, opposite meaning)]\n",
      "ROUGE-1:  0.9231\n",
      "ROUGE-2:  0.7273\n",
      "ROUGE-L:  0.9231\n",
      "ROUGE-Lsum: 0.9231\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "res_normal = rouge.compute(predictions=[prediction_normal], references=[reference])\n",
    "res_para = rouge.compute(predictions=[prediction_paraphrase], references=[reference])\n",
    "res_neg = rouge.compute(predictions=[prediction_negated], references=[reference])\n",
    "\n",
    "def show(tag, res):\n",
    "    print(f\"\\n[{tag}]\")\n",
    "    print(f\"ROUGE-1:  {res['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2:  {res['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L:  {res['rougeL']:.4f}\")\n",
    "    print(f\"ROUGE-Lsum: {res['rougeLsum']:.4f}\")\n",
    "\n",
    "show(\"Normal (near-identical wording)\", res_normal)\n",
    "show(\"Failure: paraphrase (semantic match, low surface overlap)\", res_para)\n",
    "show(\"Failure: negation (surface match, opposite meaning)\", res_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47a92c",
   "metadata": {},
   "source": [
    "Higher ROUGE scores generally indicate better content overlap with the reference, but like BLEU, ROUGE is still surface-based and does not measure semantic correctness or factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6fc57",
   "metadata": {},
   "source": [
    "### Model-based metrics: bert_score\n",
    "\n",
    "**BERTScore** is a *model-based evaluation metric* that measures the similarity between generated and reference texts using contextual embeddings from pretrained language models such as **BERT** or **RoBERTa**.  \n",
    "\n",
    "Instead of comparing surface-level n-gram overlap (like BLEU or ROUGE), BERTScore computes **semantic similarity** between words by aligning their embeddings in a high-dimensional space.  It captures meaning even when different words or phrases are used.\n",
    "\n",
    "\n",
    "üí° **Note:**  \n",
    "BERTScore relies on a large pretrained model, so it is computationally heavier than BLEU or ROUGE, but it provides a more meaning-aware evaluation of generated text.\n",
    "\n",
    "You can refer to Tianyi‚Äôs paper for more details (but unfortunately, it‚Äôs a different Tianyi ‚Äî not me ü§°): https://arxiv.org/pdf/1904.09675\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9ffb8",
   "metadata": {},
   "source": [
    "We still use the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0634e38f",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "reference = \"The cat is on the mat.\"\n",
    "\n",
    "prediction_paraphrase = \"A cat sits on the rug.\"\n",
    "prediction_negated = \"The cat is not on the mat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a30a9f",
   "metadata": {},
   "source": [
    "BERTScore uses contextual embeddings from a pretrained model (like BERT) to measure **semantic similarity** between tokens in the prediction and reference sentences.  \n",
    "Here‚Äôs how each component is computed:\n",
    "\n",
    "1. **Token-level similarity:**  \n",
    "   Each token is represented as a vector embedding.  \n",
    "   The similarity between two tokens is measured using **cosine similarity**.\n",
    "\n",
    "2. **Precision (P):**  \n",
    "   For each token in the *prediction*, find the **most similar** token in the *reference*,  \n",
    "   then take the **average** of these maximum similarities.  \n",
    "\n",
    "\n",
    "3. **Recall (R):**  \n",
    "   For each token in the *reference*, find the **most similar** token in the *prediction*,  \n",
    "   then take the average of these maximum similarities.  \n",
    "\n",
    "4. **F1 Score:**  \n",
    "   The harmonic mean of Precision and Recall, capturing overall alignment between prediction and reference\n",
    "\n",
    "\n",
    "üí° **Intuition:**  \n",
    "- Precision measures how *relevant* the generated tokens are to the reference.  \n",
    "- Recall measures how much of the reference meaning is *covered* by the generation.  \n",
    "- F1 balances both ‚Äî higher F1 indicates stronger semantic similarity overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdb5a3",
   "metadata": {},
   "source": [
    "------------\n",
    "**`TODO:`** Compute the BLEU score for the `predictions` list against the `references` list using the `bleu.compute()` function.  \n",
    "- Use the `predictions` variable as the input for the `predictions` parameter.  \n",
    "- Use the `references` variable as the input for the `references` parameter.  \n",
    "- Store the result in a variable named `results`.  \n",
    "- Print the BLEU score from the `results` dictionary using the key `'bleu'`.  \n",
    "This will help you understand how BLEU evaluates the overlap between the generated and reference texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff9bdba2",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Normal (identical)]\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1 Score:  1.0000\n",
      "\n",
      "[Paraphrase (semantic match, different words)]\n",
      "Precision: 0.7715\n",
      "Recall:    0.7715\n",
      "F1 Score:  0.7715\n",
      "\n",
      "[Negated (surface similar, opposite meaning)]\n",
      "Precision: 0.9052\n",
      "Recall:    0.9432\n",
      "F1 Score:  0.9238\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Compute BERTScore for each prediction\n",
    "res_normal = bertscore.compute(predictions=[prediction_normal], references=[reference], model_type=\"bert-base-uncased\")\n",
    "res_para = bertscore.compute(predictions=[prediction_paraphrase], references=[reference], model_type=\"bert-base-uncased\")\n",
    "res_neg = bertscore.compute(predictions=[prediction_negated], references=[reference], model_type=\"bert-base-uncased\")\n",
    "\n",
    "def show(tag, res):\n",
    "    print(f\"\\n[{tag}]\")\n",
    "    print(f\"Precision: {res['precision'][0]:.4f}\")\n",
    "    print(f\"Recall:    {res['recall'][0]:.4f}\")\n",
    "    print(f\"F1 Score:  {res['f1'][0]:.4f}\")\n",
    "\n",
    "show(\"Normal (identical)\", res_normal)\n",
    "show(\"Paraphrase (semantic match, different words)\", res_para)\n",
    "show(\"Negated (surface similar, opposite meaning)\", res_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c132a9",
   "metadata": {},
   "source": [
    "----\n",
    "**TODO** : Discussion: Why does `prediction_negated` still get a high score? How to imporve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc85c9e",
   "metadata": {},
   "source": [
    "BERTScore measures similarity between individual tokens.\n",
    "In \"The cat is not on the mat.\", all tokens except ‚Äúnot‚Äù are identical to the reference,\n",
    "so their cosine similarities are very high.\n",
    "\n",
    "Metrics like BLEURT tend to correlate more strongly with human judgments and are usually better than BERTScore at penalizing semantic errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp25-exam",
   "language": "python",
   "name": "exam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
