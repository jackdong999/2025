{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f3ce31",
   "metadata": {},
   "source": [
    "# Mini GPT: Building a Language Model from Scratch\n",
    "\n",
    "In this session, we will **build and train a simplified GPT-like model** step by step (similar to Karpathy's `nanoGPT`).  \n",
    "By the end, you will understand the **core building blocks** behind modern large language models.\n",
    "\n",
    "### What to expect\n",
    "- Learn how text is converted into numbers the model can understand.  \n",
    "- Explore how Transformers work: **embeddings, self-attention, feedforward layers, and residual connections**.  \n",
    "- Put these pieces together into a small GPT model.  \n",
    "- Train the model on Shakespeareâ€™s text.  \n",
    "- Generate new, Shakespeare-like text from scratch.  \n",
    "\n",
    "âš¡ This is a **hands-on educational exercise**: the model is tiny compared to real GPTs, but it captures the **same core ideas**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec0493",
   "metadata": {},
   "source": [
    "### Setup and Hyperparameters\n",
    "\n",
    "We start by importing **PyTorch** (`torch`) and some of its modules:\n",
    "\n",
    "- `torch.nn`: building blocks for neural networks.\n",
    "- `torch.nn.functional`: activation functions, loss functions, and other utilities.\n",
    "\n",
    "Then we define our **hyperparameters** â€” these control how the model is trained and how large it is:\n",
    "\n",
    "- `batch_size = 64`: how many training sequences we process at the same time.\n",
    "- `block_size = 256`: the maximum length of context (how many previous characters the model can \"see\" when predicting the next one).\n",
    "- `max_iters = 5000`: number of training steps.\n",
    "- `eval_interval = 500`: how often we evaluate model performance on validation data.\n",
    "- `learning_rate = 3e-4`: how fast the optimizer updates weights.\n",
    "- `device`: whether to use GPU (`cuda`) or CPU.\n",
    "- `eval_iters = 200`: number of mini-batches used to estimate loss during evaluation.\n",
    "- `n_embd = 384`: size of the embedding vectors (each token is represented by a vector of this length).\n",
    "- `n_head = 6`: number of self-attention heads in the Transformer.\n",
    "- `n_layer = 6`: number of Transformer blocks stacked together.\n",
    "- `dropout = 0.2`: fraction of neurons randomly â€œdroppedâ€ during training to prevent overfitting.\n",
    "\n",
    "These values define both the **capacity** of the model and the **training process**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d84e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94e0da",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "1. **Random seed**  \n",
    "   `torch.manual_seed(1337)` ensures reproducibility â€” weâ€™ll get the same random numbers each run.\n",
    "\n",
    "2. **Load text**  \n",
    "   We read Shakespeareâ€™s works (`input.txt`) into a single string called `text`.\n",
    "\n",
    "3. **Vocabulary**  \n",
    "   - `chars = sorted(list(set(text)))`: all unique characters that appear in the dataset.  \n",
    "   - `vocab_size`: total number of unique characters.  \n",
    "     Example: for Shakespeare itâ€™s usually around 65 (letters, punctuation, space, etc.).\n",
    "\n",
    "4. **Character â†” Integer mapping**  \n",
    "   - `stoi` (â€œstring to integerâ€): maps each character to an index.  \n",
    "   - `itos` (â€œinteger to stringâ€): reverse mapping.  \n",
    "   - `encode(s)`: converts a string into a list of integers.  \n",
    "   - `decode(l)`: converts a list of integers back into text.\n",
    "\n",
    "   > This is how we turn raw text into numerical data that a neural network can process.\n",
    "\n",
    "5. **Dataset split**  \n",
    "   - Encode the entire text into a `torch.tensor` of integers.  \n",
    "   - Use the first 90% as `train_data`.  \n",
    "   - Keep the last 10% as `val_data` (to measure generalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f3d20",
   "metadata": {},
   "source": [
    "### Creating Training Batches\n",
    "\n",
    "`get_batch(split)` samples random chunks of text:\n",
    "\n",
    "- Picks `batch_size` random starting positions.  \n",
    "- Builds input `x` (current characters) and target `y` (the same sequence shifted by one character).  \n",
    "- Moves tensors to the right device (CPU/GPU).\n",
    "\n",
    "Result: `x, y` have shape `(batch_size, block_size)` and are used to train the model to predict the **next character**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986882b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b922f",
   "metadata": {},
   "source": [
    "### Estimating Loss\n",
    "\n",
    "`estimate_loss()` checks how well the model is doing without updating weights:\n",
    "\n",
    "- Disables gradient tracking (`@torch.no_grad()` â†’ faster, less memory).  \n",
    "- Switches model to evaluation mode (`model.eval()` disables dropout).  \n",
    "- Runs `eval_iters` batches for both **train** and **val** sets.  \n",
    "- Collects and averages the losses.  \n",
    "- Switches back to training mode (`model.train()`).  \n",
    "\n",
    "This gives a stable estimate of training vs validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ec63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bb7b7",
   "metadata": {},
   "source": [
    "### Self-Attention Head\n",
    "\n",
    "A single head of self-attention:\n",
    "\n",
    "- Projects input `x` into **queries**, **keys**, and **values**.  \n",
    "- Computes attention scores (`q @ k^T`), scaled for stability.  \n",
    "- Applies a **causal mask** (`tril`) so tokens canâ€™t look ahead.  \n",
    "- Softmax â†’ probabilities over past positions.  \n",
    "- Uses these to weight the values â†’ output.\n",
    "\n",
    "Input shape: `(batch, time, channels)`  \n",
    "Output shape: `(batch, time, head_size)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6a57c",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one head, we use several in parallel:\n",
    "\n",
    "- Each `Head` learns different attention patterns.  \n",
    "- Their outputs are concatenated (`torch.cat`) along the channel dimension.  \n",
    "- A final linear projection mixes them back into the embedding size (`n_embd`).  \n",
    "- Dropout is applied for regularization.\n",
    "\n",
    "This allows the model to attend to different types of relationships at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7050f5",
   "metadata": {},
   "source": [
    "### FeedForward Layer\n",
    "\n",
    "A position-wise MLP applied to each token:\n",
    "\n",
    "- Expands embedding size (`n_embd â†’ 4*n_embd`).  \n",
    "- Applies ReLU non-linearity.  \n",
    "- Projects back down to `n_embd`.  \n",
    "- Adds dropout for regularization.\n",
    "\n",
    "This gives the model extra capacity beyond attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ada9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca679d8",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Each block combines **communication (attention)** and **computation (MLP)**:\n",
    "\n",
    "1. Apply LayerNorm â†’ Multi-Head Attention â†’ add residual connection.  \n",
    "2. Apply LayerNorm â†’ FeedForward â†’ add residual connection.  \n",
    "\n",
    "This structure lets tokens share information while keeping stable gradients.  \n",
    "Stacking several blocks builds the full Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd96d9",
   "metadata": {},
   "source": [
    "### GPTLanguageModel\n",
    "\n",
    "This class assembles the full Transformer-based language model:\n",
    "\n",
    "- **Token embeddings**: turn character indices into vectors.  \n",
    "- **Position embeddings**: add information about order in the sequence.  \n",
    "- **Stack of Transformer blocks**: attention + feedforward layers.  \n",
    "- **Final LayerNorm + Linear head**: map hidden states to vocabulary logits.  \n",
    "\n",
    "**Forward pass**:\n",
    "1. Look up token + position embeddings â†’ combine them.  \n",
    "2. Pass through Transformer blocks.  \n",
    "3. Project to logits over the vocabulary.  \n",
    "4. If targets are provided, compute cross-entropy loss.  \n",
    "\n",
    "Output: `(logits, loss)` where  \n",
    "- `logits`: predictions for next token.  \n",
    "- `loss`: training signal (or `None` if not given).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0818a1",
   "metadata": {},
   "source": [
    "### GPTLanguageModel\n",
    "\n",
    "This class assembles the full Transformer-based language model:\n",
    "\n",
    "- **Token embeddings**: turn character indices into vectors.  \n",
    "- **Position embeddings**: add information about order in the sequence.  \n",
    "- **Stack of Transformer blocks**: attention + feedforward layers.  \n",
    "- **Final LayerNorm + Linear head**: map hidden states to vocabulary logits.  \n",
    "\n",
    "**Forward pass**:\n",
    "1. Look up token + position embeddings â†’ combine them.  \n",
    "2. Pass through Transformer blocks.  \n",
    "3. Project to logits over the vocabulary.  \n",
    "4. If targets are provided, compute cross-entropy loss.  \n",
    "\n",
    "Output: `(logits, loss)` where  \n",
    "- `logits`: predictions for next token.  \n",
    "- `loss`: training signal (or `None` if not given).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99904281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop idx to the last block_size tokens\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        # get the predictions\n",
    "        logits, loss = self(idx_cond)\n",
    "        # focus only on the last time step\n",
    "        logits = logits[:, -1, :] # becomes (B, C)\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        # append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b164",
   "metadata": {},
   "source": [
    "### Model Initialization and Optimizer\n",
    "\n",
    "- Create the model (`GPTLanguageModel`) and move it to the right device (CPU/GPU).  \n",
    "- Print the total number of parameters (in millions) â†’ shows model size.  \n",
    "- Define the optimizer: **AdamW**, a variant of Adam with weight decay, commonly used for Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd242a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e916ad",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For `max_iters` steps:\n",
    "\n",
    "1. **Evaluate periodically**: every `eval_interval` steps (or at the end), estimate train/val loss.  \n",
    "2. **Get a batch**: `(xb, yb)` input and target sequences.  \n",
    "3. **Forward pass**: compute `logits` and `loss`.  \n",
    "4. **Backward pass**:  \n",
    "   - Reset gradients (`optimizer.zero_grad`).  \n",
    "   - Backpropagate (`loss.backward()`).  \n",
    "   - Update weights (`optimizer.step()`).  \n",
    "\n",
    "This loop gradually teaches the model to predict the next character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d97ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41b937",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "- Start with a single token (`0`) as the initial context.  \n",
    "- Call `model.generate(...)` to autoregressively sample the next tokens, up to `max_new_tokens=500`.  \n",
    "- Decode the generated token IDs back into text and print it.  \n",
    "\n",
    "ðŸ‘‰ The model produces new Shakespeare-like text, character by character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8df84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f7bc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How This Mini GPT Differs from Real GPT Models\n",
    "\n",
    "Our model is inspired by GPT, but itâ€™s much smaller and simpler.  \n",
    "Here are five key differences:\n",
    "\n",
    "1. **Scale**  \n",
    "   - GPT (e.g., GPT-3) has **billions of parameters**, trained on huge datasets with thousands of GPUs.  \n",
    "   - Our mini GPT has only a few **million parameters**, trained on a tiny dataset (Shakespeare) with a single GPU/CPU.\n",
    "\n",
    "2. **Tokenizer**  \n",
    "   - GPT uses **Byte Pair Encoding (BPE)** or similar subword tokenization â†’ efficient and works for all languages.  \n",
    "   - Our version uses **character-level tokens** â†’ simpler, but less efficient for large vocabularies.\n",
    "\n",
    "3. **Architecture Details**  \n",
    "   - GPT includes improvements like **pre-layer normalization**, **rotary embeddings**, and optimized attention implementations (e.g. FlashAttention).  \n",
    "   - Our mini GPT uses a **basic Transformer** with embeddings, attention, and feedforward layers.\n",
    "\n",
    "4. **Training Setup**  \n",
    "   - GPT is trained with **massive compute budgets** for weeks or months.  \n",
    "   - Our model trains in **hours** (or less) on a laptop GPU.\n",
    "\n",
    "5. **Purpose**  \n",
    "   - GPT is built for **general-purpose applications** (chat, coding, search, etc.).  \n",
    "   - Our mini GPT is designed for **learning and experimentation** â€” to show the core ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01225c",
   "metadata": {},
   "source": [
    "# MCQ: Tuesday with the slides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
