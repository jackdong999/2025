{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3dc8020",
   "metadata": {},
   "source": [
    "# üß© Reasoning with LLMs: Solving the Game of 24\n",
    "\n",
    "Welcome to this module on **reasoning frameworks for Large Language Models**.  \n",
    "In this notebook, we‚Äôll explore how different prompting and inference strategies influence an LLM‚Äôs ability to solve a classic math reasoning task: the **Game of 24**.\n",
    "\n",
    "You‚Äôll work with:\n",
    "\n",
    "- A dataset of 24-puzzles  \n",
    "- Verification tools to check whether a solution is valid  \n",
    "- Groq as our inference engine for fast, iterative reasoning  \n",
    "- Several reasoning frameworks, including:  \n",
    "  - **Few-Shot Prompting**  \n",
    "  - **Chain of Thought (CoT)**  \n",
    "  - **ReAct (Reason + Act)**  \n",
    "  - **Tree of Thoughts (ToT) ‚Äî BFS variant**\n",
    "\n",
    "As you progress, you‚Äôll implement each framework yourself and compare how they perform on the same underlying task.  \n",
    "By the end, you‚Äôll have hands-on experience with multiple LLM reasoning paradigms and a deeper understanding of how structured prompting can guide models toward more reliable problem-solving.\n",
    "\n",
    "Let‚Äôs get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c39de0",
   "metadata": {},
   "source": [
    "## üìò The Game of 24 Dataset\n",
    "\n",
    "The **Game of 24** is a classic arithmetic puzzle: Given **four numbers**, your goal is to combine them using the operations **+**, **‚àí**, **√ó**, and **√∑** (and parentheses) to make the value **24**.\n",
    "\n",
    "For example:  \n",
    "- Numbers: `4 7 8 8`  \n",
    "- One solution: `(7 - 4) √ó 8 = 24`\n",
    "\n",
    "### üóÇÔ∏è What‚Äôs in the Dataset?\n",
    "\n",
    "The *Game of 24 dataset* consists of many such **four-number puzzles**, typically drawn from the range 1‚Äì9. Each entry commonly includes:\n",
    "- The **four digits** that make up the puzzle  \n",
    "- Some descriptive statistics drawn from *[https://www.4nums.com/](https://www.4nums.com/)* (this is where the dataset was extracted from)\n",
    "\n",
    "Not all sets of numbers can make 24‚Äîpart, but all entries in this dataset do!\n",
    "\n",
    "### üîç In This Notebook\n",
    "\n",
    "The dataset has been downloaded the code to load it is given below. If you're curious feel free to explore the dataset on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dataset_game24.csv.gz\", compression=\"gzip\")\n",
    "display(df.head())\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b4592",
   "metadata": {},
   "source": [
    "**`Discussion:`** Solve the first two puzzles on your own to get some intuition of the game.\n",
    "- 1 1 4 6\n",
    "- 1 1 11 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29428949",
   "metadata": {},
   "source": [
    "\\[Your Answer\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b98b2",
   "metadata": {},
   "source": [
    "### üß™ Verifying Student Solutions Programmatically\n",
    "\n",
    "To help you check whether a proposed solution to a Game of 24 puzzle is valid, we provide a small evaluation utility.  \n",
    "You won‚Äôt need to understand every detail of the code right away, just how to use it.\n",
    "\n",
    "#### üîç What the Verifier Does\n",
    "\n",
    "The evaluation functions perform three main checks:\n",
    "\n",
    "1. **Is the solution ‚Äúfinal‚Äù?**  \n",
    "   The code checks whether your sequence of steps ends in a legitimate final state ‚Äî that is, you‚Äôve produced a single expression without leftover numbers.\n",
    "\n",
    "2. **Did you use the correct numbers?**  \n",
    "   Your final expression must use *exactly* the four numbers from the puzzle, no more and no fewer.\n",
    "\n",
    "3. **Does the expression equal 24?**  \n",
    "   Using SymPy, the tool evaluates your final expression safely and determines whether it results in 24.\n",
    "\n",
    "#### üß© How You‚Äôll Use It\n",
    "\n",
    "You‚Äôll call:\n",
    "\n",
    "```python\n",
    "is_terminal, reward = evaluate(steps, puzzle, current_state)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- **`steps`** is a list of strings describing the steps the student (or your algorithm) took  \n",
    "- **`puzzle`** is the original four-number puzzle as a string  \n",
    "- **`current_state`** the remaining (unsued) numbers\n",
    "\n",
    "**The function returns:**\n",
    "\n",
    "- **`is_terminal`** whether the attempt reached a final state  \n",
    "- **`reward`**\n",
    "  - `1.0` if the final expression correctly evaluates to 24  \n",
    "  - `0.0` otherwise  \n",
    "\n",
    "\n",
    "### ‚úîÔ∏è Example\n",
    "\n",
    "```python\n",
    "steps = [\"1 * 1 = 1 (left: 1 3 8)\", \"1 * 3 = 3 (left: 3 8)\", \"3 * 8 = 24 (left: 24)\", \"Answer: (((1 * 1) * 3) * 8) = 24\"]\n",
    "puzzle = \"1 1 3 8\"\n",
    "current_state = \"24\"\n",
    "\n",
    "evaluate(steps, puzzle, current_state)\n",
    "# ‚Üí (True, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sympy import simplify\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def is_final(steps: List[str], current_state: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether the transition sequence ends in a final state.\n",
    "\n",
    "    A state is considered final if:\n",
    "    - There is at least one step\n",
    "    - The last step does not contain the word \"left\"\n",
    "    - The current_state string contains only one token\n",
    "    \"\"\"\n",
    "    if not steps:\n",
    "        return False\n",
    "\n",
    "    expression = steps[-1]\n",
    "\n",
    "    if \"left\" in expression:\n",
    "        return False\n",
    "\n",
    "    # current_state should be a single token (no spaces)\n",
    "    if len(current_state.split()) > 1:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    steps: List[str],\n",
    "    puzzle: str,\n",
    "    current_state: str\n",
    ") -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the puzzle result in a fully self-contained way.\n",
    "\n",
    "    Returns:\n",
    "        (is_terminal_state: bool, reward: float)\n",
    "\n",
    "    Reward rules:\n",
    "    - If final and expression uses incorrect numbers ‚Üí reward 0\n",
    "    - If final and evaluates to 24 ‚Üí reward 1\n",
    "    - Otherwise ‚Üí reward 0\n",
    "    \"\"\"\n",
    "    final = is_final(steps, current_state)\n",
    "\n",
    "    if not final or not steps or not steps[-1]:\n",
    "        return False, 0.0\n",
    "\n",
    "    # Extract the expression from the final step\n",
    "    expression = steps[-1].lower().replace(\"answer: \", \"\")\n",
    "    expression = expression.split(\"=\")[0]\n",
    "\n",
    "    # Numbers used in the player's expression\n",
    "    numbers_used = re.findall(r\"\\d+\", expression)\n",
    "\n",
    "    # Numbers available in the original puzzle\n",
    "    puzzle_numbers = re.findall(r\"\\d+\", puzzle)\n",
    "\n",
    "    # Check whether they used exactly the correct numbers\n",
    "    if sorted(numbers_used) != sorted(puzzle_numbers):\n",
    "        print(\"Incorrect numbers used\")\n",
    "        return True, 0.0\n",
    "\n",
    "    # Safely evaluate expression\n",
    "    try:\n",
    "        correct = simplify(expression) == 24\n",
    "        print(\"Expression is correct: \", bool(correct))\n",
    "        return True, float(correct)\n",
    "    except Exception:\n",
    "        print(\"Error evaluating expression\")\n",
    "        return True, 0.0\n",
    "\n",
    "# Example usage\n",
    "steps = [\"1 * 1 = 1 (left: 1 3 8)\", \"1 * 3 = 3 (left: 3 8)\", \"3 * 8 = 24 (left: 24)\", \"Answer: (((1 * 1) * 3) * 8) = 24\"]\n",
    "puzzle = \"1 1 3 8\"\n",
    "current_state = \"24\"\n",
    "\n",
    "evaluate(steps, puzzle, current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce2e6d",
   "metadata": {},
   "source": [
    "## üöÄ Inference Engine: Using Groq\n",
    "\n",
    "For all of our **inference-based reasoning steps**, we‚Äôll be using **Groq** as our model host and execution engine.  \n",
    "\n",
    "If you haven‚Äôt used Groq before, don‚Äôt worry, the setup and usage should already be familiar from earlier exercises in this course.  \n",
    "Feel free to revisit those notebooks if you need a quick refresher on how to make inference calls or format prompts.\n",
    "\n",
    "In the next cell, we‚Äôre adding a simple example to show how to make an inference call with Groq. This will serve as a quick reference before we start using Groq more extensively in our reasoning frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Very briefly, explain reasoning frameworks in large language models.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    max_completion_tokens=512\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987adb7",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Helper Function: `generate`\n",
    "\n",
    "To keep our code clean and avoid repeating the same inference boilerplate, we define a small helper function called `generate`.\n",
    "\n",
    "This function simply sends a prompt to Groq, retrieves the model‚Äôs response, and returns the generated text.  \n",
    "You can call it with a prompt, and optionally adjust the model, temperature, or maximum output length.\n",
    "\n",
    "We'll use `generate(...)` throughout the notebook to make inference calls concise and readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0eef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "        prompt: str, \n",
    "        model: str=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        max_completion_tokens: int=128,\n",
    "        temperature: float=0.7\n",
    "    ) -> str:\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691a76c",
   "metadata": {},
   "source": [
    "## üß† Solving the Game of 24 with LLM Reasoning Frameworks\n",
    "\n",
    "Now that you‚Äôre familiar with the dataset, the evaluation utilities, and our inference engine (Groq), it‚Äôs time to start exploring how **large language models** can approach the Game of 24.\n",
    "\n",
    "Over the next sections, we‚Äôll experiment with several **reasoning frameworks**, structured prompting strategies that help LLMs break down problems, plan steps, and produce more reliable solutions.\n",
    "\n",
    "### üîπ First Framework: Few-Shot Prompting\n",
    "\n",
    "We‚Äôll begin with **few-shot prompting**, where we provide the model with a handful of example puzzles and their solutions.  \n",
    "These examples act as a guide, helping the model infer the **expected structure** and **reasoning process** before attempting new puzzles on its own.\n",
    "\n",
    "This will give us a baseline for how well an LLM can solve Game of 24 puzzles when primed with good examples.\n",
    "\n",
    "Since this is the first and easiet example, we provide the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f928642",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = '''Use numbers and basic arithmetic operations (+ - * /) to obtain 24. Follow the exact format of the following examples and provide just one answer. Do not explain simply list the final expression that reaches 24.\n",
    "\n",
    "Example: 4 4 6 8\n",
    "Answer: (4 + 8) * (6 - 4) = 24\n",
    "\n",
    "Example: 2 9 10 12\n",
    "Answer: 2 * 12 * (10 - 9) = 24\n",
    "\n",
    "Example: 4 9 10 13\n",
    "Answer: (13 - 9) * (10 - 4) = 24\n",
    "\n",
    "Example: 1 4 8 8\n",
    "Answer: (8 / 4 + 1) * 8 = 24\n",
    "\n",
    "Example: 5 5 5 9\n",
    "Answer: 5 + 5 + 5 + 9 = 24\n",
    "\n",
    "Input: {input}\n",
    "'''\n",
    "\n",
    "state = dataset[0][\"Puzzles\"]\n",
    "prompt = io.format(input=state)\n",
    "response = generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a669ec",
   "metadata": {},
   "source": [
    "### üîπ Second Framework: Chain of Thought (CoT)\n",
    "\n",
    "Next, we‚Äôll build on top of the few-shot approach by introducing **Chain of Thought reasoning**.  \n",
    "While few-shot prompting provides the model with examples of *what* a correct solution looks like, Chain of Thought helps the model understand *how* to get there.\n",
    "\n",
    "With Chain of Thought, we explicitly ask the model to show its intermediate reasoning steps.  \n",
    "For the Game of 24, this means the model won‚Äôt just output a final expression, it will walk through the arithmetic decisions that lead to the solution.\n",
    "\n",
    "Adding CoT on top of few-shot prompting will allow us to see whether providing structured reasoning examples improves the model‚Äôs accuracy and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea94e2",
   "metadata": {},
   "source": [
    "**`TODO:`** Create a new prompt that includes your few-shot examples **plus explicit step-by-step reasoning** for each one, then call Groq and verify the model's output just as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882ec7ea",
   "metadata": {},
   "source": [
    "### üîπ Third Framework: ReAct (Reason + Act)\n",
    "\n",
    "After exploring Chain of Thought, we‚Äôll move to a more interactive and modular reasoning framework: **ReAct**.\n",
    "\n",
    "**ReAct** combines two key capabilities:\n",
    "- **Reasoning** where the model explains its thought process step by step  \n",
    "- **Acting** where the model takes actions such as querying tools, updating its internal state, or performing calculations\n",
    "\n",
    "For the Game of 24, this means the model can:\n",
    "1. Reason about which numbers to combine  \n",
    "2. ‚ÄúAct‚Äù by generating intermediate expressions  \n",
    "3. Reflect on whether those actions bring it closer to 24  \n",
    "4. Continue the cycle until it reaches a final answer\n",
    "\n",
    "This framework is especially interesting because it simulates an iterative problem-solving process rather than producing the solution in one pass.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476419ba",
   "metadata": {},
   "source": [
    "**`TODO:`** Design a prompt that demonstrates the **ReAct** pattern (thinking steps + actions taken). As mentioned above, the action should combine two of the available numbers to produce a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a3accd8",
   "metadata": {},
   "source": [
    "#### üîç Parsing Intermediate States in ReAct\n",
    "\n",
    "Because the **ReAct** framework unfolds step-by-step, we need a way to extract the model‚Äôs *current state* after each action it produces.  \n",
    "\n",
    "Therefore we need to define a helper function that handles this: it reads the model‚Äôs action string, identifies whether it‚Äôs selecting numbers or producing an expression, and returns the updated state.  \n",
    "\n",
    "\n",
    "For example consider the input `2 8 8 14`. Imagine the action is the following: `14 - 8 = 6 (left: 2 8 6)`. The current state is `2 8 6`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7f59a",
   "metadata": {},
   "source": [
    "**`TODO:`** Complete the `parse_state` function which as the name dictates, parses the state from a given action.\n",
    "\n",
    "*Note:* There is not a unique solution to this exercise. As you can imagine the parsing function heavily depends on (1) the model you're using and (2) the prompt you have previously defined. The point is that you come up with a prompting + parsing combination that works for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0250820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(action: str):\n",
    "    \"\"\"\n",
    "    Given an action string from the model, parse and return the current state.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parsing logic based on the action format\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c0ecb",
   "metadata": {},
   "source": [
    "#### The Full ReAct Framework\n",
    "\n",
    "Now that you have a way to parse intermediate states, you‚Äôre ready to implement the complete **ReAct** loop.\n",
    "\n",
    "Your implementation should:\n",
    "\n",
    "- Iteratively send the current state back to the model  \n",
    "- Parse each action using the function from above  \n",
    "- Update the state at every step  \n",
    "- Decide when to stop the loop  \n",
    "\n",
    "While coding, think carefully about:\n",
    "\n",
    "- **How many steps** the agent should be allowed to take for a 4-number Game of 24 puzzle  \n",
    "- **What should happen if the current state becomes `24`** before exceeding the step limit  \n",
    "- **How to detect impossible or looping behavior**  \n",
    "\n",
    "**`TODO:`** Build a full ReAct solver that progresses step-by-step until it either finds 24 or exhausts its allowed reasoning budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebfb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "651412eb",
   "metadata": {},
   "source": [
    "### üîπ Fourth Framework: Tree of Thoughts (ToT)\n",
    "\n",
    "Now we move to **Tree of Thoughts**, a more powerful reasoning framework that lets the model explore **multiple reasoning paths** rather than committing to a single line of thought.\n",
    "\n",
    "Tree of Thoughts treats intermediate reasoning steps as **‚Äúthought nodes‚Äù** in a search tree.  \n",
    "At each step, the model can propose several possible moves, and we can choose which branches to keep exploring.\n",
    "\n",
    "#### üå≥ How ToT Works \n",
    "\n",
    "For the Game of 24, each ‚Äúthought‚Äù represents one arithmetic operation combining two numbers. A single ToT iteration typically involves:\n",
    "\n",
    "1. **Generating candidate thoughts**  \n",
    "   The model proposes several possible intermediate expressions.\n",
    "\n",
    "2. **Evaluating those thoughts**  \n",
    "   Each candidate is scored based on how promising it looks (e.g., whether it simplifies the puzzle).\n",
    "\n",
    "3. **Expanding the search**  \n",
    "   Promising thoughts become new states, which the model expands in the next round.\n",
    "\n",
    "#### üîé Our Strategy: Breadth-First Search (BFS) ToT\n",
    "\n",
    "Tree of Thoughts can be implemented in different search styles (DFS, BFS, heuristic search, etc.).  \n",
    "In this notebook, **we will use the BFS version**, which:\n",
    "\n",
    "- Expands all thoughts at a given depth before moving deeper  \n",
    "- Ensures we explore a broad range of possibilities early  \n",
    "- Helps avoid getting stuck in a poor branch too quickly  \n",
    "- Works well for relatively small search spaces like a 4-number puzzle\n",
    "\n",
    "Using BFS gives us a controlled, systematic way to observe how the model reasons when allowed to explore multiple possibilities simultaneously.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bcfcf",
   "metadata": {},
   "source": [
    "#### üîÑ Generation & Expansion in ToT-BFS\n",
    "\n",
    "In the **generation phase**, the model proposes several possible ‚Äúthoughts‚Äù (intermediate moves) for the current puzzle state.  \n",
    "During the **expansion phase**, each of these thoughts becomes a new node in the search tree, and you repeat the process for all nodes at the current depth.\n",
    "\n",
    "Because we‚Äôre using **BFS**, the algorithm:\n",
    "\n",
    "- Generates thoughts for *every* node at the current depth  \n",
    "- Collects all resulting child states  \n",
    "\n",
    "This ensures a wide exploration of possibilities before diving deeper, helping the solver avoid early mistakes and discover more promising reasoning paths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6eb0f",
   "metadata": {},
   "source": [
    "**`TODO:`** To make this easier and to spare you from heavy prompt engineering, we provide a **ready-to-use BFS prompt template**. Nevertheless, feel free to experiment with your own adaptations or examine how the prompt interacts under different samples, models or temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs = '''Use numbers and basic arithmetic operations (+ - * /). Each step, you are only allowed to choose two of the remaining numbers to obtain a new number.  Follow the format of the following examples. Do not explain simply list possible next steps as well as all the remaining numbers and nothing else.\n",
    "\n",
    "Example: 2 8 8 14\n",
    "Possible next steps:\n",
    "2 + 8 = 10 (left: 8 10 14)\n",
    "8 / 2 = 4 (left: 4 8 14)\n",
    "14 + 2 = 16 (left: 8 8 16)\n",
    "2 * 8 = 16 (left: 8 14 16)\n",
    "8 - 2 = 6 (left: 6 8 14)\n",
    "\n",
    "Example: 1 3\n",
    "Possible next steps:\n",
    "1 + 3 = 4 (left: 4)\n",
    "1 * 3 = 3 (left: 3)\n",
    "3 - 1 = 2 (left: 2)\n",
    "3 / 1 = 3 (left: 3)\n",
    "1 - 3 = -2 (left: -2)\n",
    "\n",
    "Input: {input}\n",
    "Possible next steps:\n",
    "'''\n",
    "\n",
    "state = dataset[0][\"Puzzles\"]\n",
    "prompt = bfs.format(input=state)\n",
    "response = generate(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b926ad",
   "metadata": {},
   "source": [
    "**`TODO:`** The `bfs` prompt suggest multiple next steps in one response. Complete the following function to load all the suggestions in a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558130f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bfs_response(response: str, state: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse the BFS response into a list of proposals.\n",
    "    If the state is not \"24\", ensure that each proposal ends with a closing parenthesis.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parsing logic\n",
    "    ...\n",
    "\n",
    "proposals = parse_bfs_response(response, state)\n",
    "print(proposals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852d740",
   "metadata": {},
   "source": [
    "#### üßÆ Evaluation Phase in ToT-BFS\n",
    "\n",
    "After generating candidate thoughts at the current depth, the **evaluation phase** determines which of them are worth expanding further.\n",
    "\n",
    "In this phase, the algorithm:\n",
    "\n",
    "- Assess each thought using a scoring function or heuristic  \n",
    "- Filter out unpromising or invalid states  \n",
    "- Keep only the thoughts that appear most promising for the next BFS layer  \n",
    "\n",
    "The goal is to maintain a manageable search frontier while ensuring that strong reasoning paths continue to propagate through the tree.  \n",
    "This evaluation step is crucial for guiding the search effectively toward a solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cde3b",
   "metadata": {},
   "source": [
    "**`TODO:`** For the same reasons as before, we provide a **ready-to-use evaluation prompt template**. Nevertheless, feel free to experiment with your own adaptations or examine how the prompt interacts under different samples, models or temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = '''Evaluate if given numbers can reach 24 by responding with the following: \"sure\", \"likely\" or \"impossible\". Follow the format of the following examples. Try to be brief.\n",
    "\n",
    "Example: 10 14\n",
    "10 + 14 = 24\n",
    "sure\n",
    "\n",
    "Example: 11 12\n",
    "11 + 12 = 23\n",
    "12 - 11 = 1\n",
    "11 * 12 = 132\n",
    "11 / 12 = 0.91\n",
    "impossible\n",
    "\n",
    "Example: 4 4 10\n",
    "4 + 4 + 10 = 8 + 10 = 18\n",
    "4 * 10 - 4 = 40 - 4 = 36\n",
    "(10 - 4) * 4 = 6 * 4 = 24\n",
    "sure\n",
    "\n",
    "Example: 4 9 11\n",
    "9 + 11 + 4 = 20 + 4 = 24\n",
    "sure\n",
    "\n",
    "Example: 5 7 8\n",
    "5 + 7 + 8 = 12 + 8 = 20\n",
    "(8 - 5) * 7 = 3 * 7 = 21\n",
    "I cannot obtain 24 now, but numbers are within a reasonable range\n",
    "likely\n",
    "\n",
    "Example: 5 6 6\n",
    "5 + 6 + 6 = 17\n",
    "(6 - 5) * 6 = 1 * 6 = 6\n",
    "I cannot obtain 24 now, but numbers are within a reasonable range\n",
    "likely\n",
    "\n",
    "Example: 10 10 11\n",
    "10 + 10 + 11 = 31\n",
    "(11 - 10) * 10 = 10\n",
    "10 10 10 are all too big\n",
    "impossible\n",
    "\n",
    "Example: 1 3 3\n",
    "1 * 3 * 3 = 9\n",
    "(1 + 3) * 3 = 12\n",
    "1 3 3 are all too small\n",
    "impossible\n",
    "\n",
    "Input: {input}\n",
    "'''\n",
    "\n",
    "evaluations = []\n",
    "for proposal in proposals:\n",
    "    prompt = evaluation.format(input=proposal)\n",
    "    response = generate(prompt)\n",
    "    evaluations.append(response)\n",
    "print(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2da1",
   "metadata": {},
   "source": [
    "### üî¢ Parsing the Evaluation Scores\n",
    "\n",
    "During the evaluation step, the model labels each thought with a descriptor indicating how promising it is.  \n",
    "To make these labels usable in our BFS logic, we **parse** them into numeric scores using:\n",
    "\n",
    "```python\n",
    "code_map = {r\"impossible\": 0.001, r\"likely\": 1, r\"sure\": 20}\n",
    "```\n",
    "\n",
    "These parsed values let us compare and rank thoughts, helping the BFS process decide which branches should continue to the next level.\n",
    "\n",
    "**`TODO:`** Complete the following function which parses the evaluations based on the above mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evaluation(evaluations: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Parse evaluation strings into numerical scores.\n",
    "    Args:\n",
    "        evaluations (List[str]): List of evaluation strings from the model.\n",
    "    Returns:\n",
    "        float: The cumulative score based on the evaluations.\n",
    "\n",
    "    Why a list of evaluations? People often evaluate the same proposal multiple times to ensure consistency.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parsing logic\n",
    "    ...\n",
    "\n",
    "evaluation_scores = [parse_evaluation([evaluation]) for evaluation in evaluations]\n",
    "evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979c4b2",
   "metadata": {},
   "source": [
    "#### üå≥ Bringing It All Together: Implementing ToT-BFS\n",
    "\n",
    "You now have all the essential building blocks for **Tree of Thoughts with Breadth-First Search**:\n",
    "\n",
    "- A structured prompt for generating candidate thoughts  \n",
    "- A generation & expansion procedure  \n",
    "- An evaluation phase to score and filter thoughts  \n",
    "- A parsing step to convert model labels into numeric scores  \n",
    "\n",
    "Your next step is to **combine these components into a complete ToT-BFS solver**.\n",
    "\n",
    "As you build it, think about:\n",
    "\n",
    "- How many thoughts to generate per node  \n",
    "- How many depth levels the BFS should explore  \n",
    "- How to maintain and update the frontier of active states  \n",
    "- When to stop the search (e.g., reaching 24 or exhausting options)\n",
    "\n",
    "**`TODO:`** Implement the ToT-BFS solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5ecec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634ba1e0",
   "metadata": {},
   "source": [
    "### üí° Hint: Reaching 24 Isn‚Äôt the End‚Ä¶ Yet\n",
    "\n",
    "In ToT-BFS, reaching a state valued at **24** means the search has found a *promising path*, but you‚Äôre not done.  \n",
    "You still need to:\n",
    "\n",
    "- Trace back the sequence of thoughts that led to this state  \n",
    "- Use those thoughts to **reconstruct the final expression**\n",
    "\n",
    "Remember: the solver must output a valid arithmetic expression, not just the number 24.  \n",
    "So once you hit 24, follow the breadcrumb trail of thoughts to build the full solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a736c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a323b290",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "You‚Äôve now explored a full spectrum of **LLM reasoning frameworks**, from simple pattern-based prompting to structured multi-branch search.  \n",
    "By applying these approaches to the Game of 24, you‚Äôve seen how different reasoning strategies shape the model‚Äôs ability to break down problems, plan actions, and arrive at correct solutions.\n",
    "\n",
    "Each framework offers its own strengths:\n",
    "\n",
    "- **Few-Shot Prompting** gives the model patterns to mimic.  \n",
    "- **Chain of Thought** encourages transparent step-by-step reasoning.  \n",
    "- **ReAct** introduces iterative decision-making with intermediate states.  \n",
    "- **Tree of Thoughts (BFS)** allows branching exploration and strategic evaluation.\n",
    "\n",
    "Together, these methods illustrate how prompting and inference design can dramatically influence model behavior‚Äîan essential concept for building reliable, controllable AI systems.\n",
    "\n",
    "\n",
    "### üìö Summary of Reasoning Frameworks\n",
    "\n",
    "Below is an expanded overview of the reasoning frameworks covered in this module, plus several influential ones you may want to explore next.  \n",
    "Each entry includes the **framework name**, its **core idea**, and a **reference** to the original paper or introduction.\n",
    "\n",
    "| Framework | Core Idea | Link |\n",
    "|----------|-----------|-----------|\n",
    "| **Few-Shot Prompting** | Guide the model by showing a handful of solved examples | [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165) |\n",
    "| **Chain of Thought (CoT)** | Provide step-by-step reasoning to improve multi-step problem solving | [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903) |\n",
    "| **ReAct** | Interleave explicit reasoning steps with actions taken in an environment |[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629) |\n",
    "| **Tree of Thoughts (ToT)** | Explore multiple reasoning paths using a structured search tree | [https://arxiv.org/abs/2305.10601](https://arxiv.org/abs/2305.10601) |\n",
    "| **Reflexion** | An agent learns from failures by reflecting on past attempts to improve future reasoning | [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) |\n",
    "| **Language Agent Tree Search (LATS)** | A search algorithm where LLMs guide tree expansion with value and policy evaluations | [https://arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406) |\n",
    "| **Graph of Thoughts (GoT)** | Represent reasoning as a graph rather than a linear or tree structure, enabling merging and recombination | [https://arxiv.org/abs/2308.09687](https://arxiv.org/abs/2308.09687)|\n",
    "| **Fleet of Agents (FoA)** | Employs a genetic-type filtering approach to dynamically navigate through the search space | [https://arxiv.org/abs/2405.06691](https://arxiv.org/abs/2405.06691) |\n",
    "\n",
    "These frameworks highlight the rapidly evolving landscape of LLM reasoning, from simple pattern learning to multi-agent systems and structured search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
