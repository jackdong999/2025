{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31e451c8",
      "metadata": {
        "id": "31e451c8"
      },
      "source": [
        "# Model Compression in Large Language Models üß†‚ö°\n",
        "\n",
        "Welcome! In this notebook, you‚Äôll explore how **large language models (LLMs)** can be made smaller, faster, and more efficient through two powerful techniques:  \n",
        "**Knowledge Distillation** and **Quantization**.\n",
        "\n",
        "You‚Äôll start by studying how a large, powerful ‚Äúteacher‚Äù model can transfer its knowledge to a smaller ‚Äústudent‚Äù model, a process known as **distillation**.  \n",
        "Then you‚Äôll see how **quantization** can further compress models by reducing numerical precision, cutting down memory use, and speeding up inference without major accuracy loss.\n",
        "\n",
        "We‚Äôll focus on the **Masked Language Modeling (MLM)** task, the same pretraining objective used for models like **BERT** and **DistilBERT**, and evaluate how both techniques preserve performance while improving efficiency.\n",
        "\n",
        "### üîç What you‚Äôll do\n",
        "- üìö **Learn key concepts** ‚Üí understand what distillation and quantization mean, and why they matter for deploying LLMs.  \n",
        "- üß© **Load a real dataset** ‚Üí use the **Yelp Polarity** dataset to work with natural, human-written text.  \n",
        "- üß† **Create masked samples** ‚Üí hide random words and challenge models to fill in the blanks.  \n",
        "- ‚öñÔ∏è **Compare teacher vs. student** ‚Üí test how well **DistilBERT** imitates **BERT**.  \n",
        "- ‚ö° **Quantize the teacher** ‚Üí run BERT in 8-bit precision using **BitsAndBytes** to measure speed and accuracy changes.  \n",
        "- üìä **Evaluate and visualize results** ‚Üí compare predictions, agreement scores, KL divergence, and runtime performance.  \n",
        "\n",
        "‚ú® By the end, you‚Äôll see how **distillation and quantization work together** to make modern language models leaner, faster, and easier to deploy ‚Äî all while keeping their intelligence intact. Let‚Äôs get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9968a7d6",
      "metadata": {
        "id": "9968a7d6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os, gc, torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e2c0759",
      "metadata": {
        "id": "0e2c0759"
      },
      "source": [
        "## üßæ Dataset: Yelp Polarity\n",
        "\n",
        "For this demo, we‚Äôll use the **Yelp Polarity** dataset, a collection of real user reviews from Yelp labeled as **positive** or **negative**.  \n",
        "Even though this dataset is designed for sentiment classification, we‚Äôll repurpose its text for **masked language modeling**.  \n",
        "\n",
        "Using real, naturally written reviews helps us test how well the teacher (**BERT**) and student (**DistilBERT**) handle everyday language and maintain consistent predictions under distillation.\n",
        "\n",
        "**`TODO:`** Load the train set of the `yelp_polarity`dataset. Only 1% shoudl suffice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48616925",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48616925",
        "outputId": "ae6e8df3-7832-47a4-e848-f0a6afc3db26"
      },
      "outputs": [],
      "source": [
        "dataset = ...\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa73f50",
      "metadata": {
        "id": "2fa73f50"
      },
      "source": [
        "### üß© Prepare Masked Sentences\n",
        "\n",
        "Your goal here is to prepare a small set of sentences that include a **[MASK]** token.  \n",
        "You‚Äôll use these sentences later to test how well BERT and DistilBERT predict missing words.\n",
        "\n",
        "\n",
        "**`TODO:`**\n",
        "1. Load the **BERT tokenizer** so you can check the token length of each sentence.  \n",
        "2. Write a function that takes a sentence, randomly selects one word, replaces it with `[MASK]`, and returns both the masked sentence and the original word.  \n",
        "3. Make sure to skip sentences that are too short or exceed **512 tokens** when tokenized.  \n",
        "4. Loop through the dataset, apply your function, and keep only the valid masked sentences.  \n",
        "5. Stop once you have collected **200 valid examples**, then store them in two lists ‚Äî one for masked sentences and one for the original words.\n",
        "\n",
        "When you‚Äôre done, print how many masked sentences you successfully created.  \n",
        "These will be your **test inputs** for the next experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b018e329",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b018e329",
        "outputId": "0943566a-c0ef-4808-858d-b723572eb51d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ef686cd1",
      "metadata": {
        "id": "ef686cd1"
      },
      "source": [
        "## ‚öôÔ∏è Distillation and Pipelins\n",
        "\n",
        "In Hugging Face‚Äôs Transformers library, **pipelines** provide a simple, high-level interface for running common NLP tasks without needing to manually handle tokenization, model inputs, or post-processing.\n",
        "\n",
        "#### Masked Language Modeling (MLM)\n",
        "- The `\"fill-mask\"` pipeline is designed for **Masked Language Modeling (MLM)** tasks.  \n",
        "- It automatically detects the `[MASK]` token in a sentence and predicts which words are most likely to fill that blank.  \n",
        "- Under the hood, the pipeline handles tokenization, model inference, and decoding ‚Äî returning readable word predictions with their probabilities.\n",
        "\n",
        "#### Teacher and Student Models\n",
        "- `pipeline(\"fill-mask\", model=\"bert-base-uncased\")` loads **BERT**, our **teacher model** ‚Äî large and highly accurate.  \n",
        "- `pipeline(\"fill-mask\", model=\"distilbert-base-uncased\")` loads **DistilBERT**, our **student model** ‚Äî smaller, faster, and distilled from BERT.\n",
        "\n",
        "In this setup, both pipelines perform the same task, allowing us to directly compare their predictions and efficiency.  \n",
        "For more details, check out the [Transformers pipeline documentation](https://huggingface.co/docs/transformers/en/main_classes/pipelines)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e02b79",
      "metadata": {
        "id": "61e02b79"
      },
      "source": [
        "**`TODO:`** Load the teacher and student models through a `pipeline` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629638f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629638f1",
        "outputId": "cb622149-40bf-4235-a683-40befc5ab718"
      },
      "outputs": [],
      "source": [
        "teacher = ...\n",
        "student = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9d11da",
      "metadata": {
        "id": "4a9d11da"
      },
      "source": [
        "### üìä Task: Compare Teacher and Student Performance\n",
        "\n",
        "Now that you have your masked sentences and both models loaded, it‚Äôs time to **evaluate how similar their predictions are** and how efficiently they run.\n",
        "\n",
        "In this section, you‚Äôll write a loop that goes through each masked sentence, asks both models to fill in the blank, and records several measurements:\n",
        "\n",
        "1. **Inference time**  \n",
        "   Measure how long each model takes to make a prediction. This helps show how much faster the student (DistilBERT) is compared to the teacher (BERT).\n",
        "\n",
        "2. **Top-1 agreement**  \n",
        "   Check if the two models predict the **same top word** for the `[MASK]` position.\n",
        "\n",
        "3. **Top-5 overlap**  \n",
        "   Compare the sets of their top-5 predictions. Count how many of the teacher‚Äôs top-5 words also appear in the student‚Äôs top-5 list.\n",
        "\n",
        "4. **KL divergence**  \n",
        "   Compute the Kullback‚ÄìLeibler divergence between the teacher‚Äôs and student‚Äôs predicted probability distributions.  \n",
        "   A lower KL value means the student‚Äôs predictions are closer to the teacher‚Äôs.\n",
        "\n",
        "After looping through all samples, calculate and print the **average results** across the dataset.  \n",
        "These metrics together will show how well the student model preserves the teacher‚Äôs knowledge while being faster to run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec37abf",
      "metadata": {
        "id": "7ec37abf"
      },
      "source": [
        "**`TODO:`** Implement the experiment as described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbbd65f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bbbd65f",
        "outputId": "c29cb879-2851-44c3-8321-e6cb94b51623"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "79f58f8e",
      "metadata": {
        "id": "79f58f8e"
      },
      "source": [
        "**`TODO:`** Plot the Top-1 Agreement and the Top-5 Overlap between the student and the teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c387fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "8c387fd9",
        "outputId": "0d059d74-16b7-4315-85dc-5617d8ce0283"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "725869a6",
      "metadata": {
        "id": "725869a6"
      },
      "source": [
        "## ‚ö° Task: Quantize the Teacher Model with Bitsandbytes\n",
        "\n",
        "Now you‚Äôll explore **quantization**\n",
        "\n",
        "In this exercise, you will:\n",
        "1. Load the original **BERT** teacher model in full precision.  \n",
        "2. Load a second version in **8-bit precision** using the `bitsandbytes` library.  \n",
        "3. Run both models on the same masked sentence.  \n",
        "4. Compare their predictions and measure the difference in inference time.\n",
        "\n",
        "Quantization reduces model size and memory use while keeping performance nearly the same.  \n",
        "It‚Äôs one of the main techniques used to deploy LLMs efficiently in real-world applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a090a0",
      "metadata": {
        "id": "d2a090a0"
      },
      "source": [
        "### ‚öôÔ∏è Introduction to BitsAndBytes Quantization\n",
        "\n",
        "**BitsAndBytes** is a library that enables efficient **low-precision inference and training** for large language models.  \n",
        "It allows you to load models in **8-bit** or **4-bit** precision directly through Hugging Face Transformers ‚Äî reducing memory use and speeding up inference without needing to retrain the model.\n",
        "\n",
        "Because BitsAndBytes performs GPU-accelerated quantization, it **requires access to a CUDA-enabled GPU**.  \n",
        "If you‚Äôre running this notebook locally without a GPU, you will encounter errors.\n",
        "We recommend running this section on **[Google Colab](https://colab.research.google.com)** (with ‚ÄúRuntime ‚Üí Change runtime type ‚Üí GPU‚Äù) or any environment with GPU support.\n",
        "\n",
        "Learn more from the official documentation:  üëâ [https://huggingface.co/docs/bitsandbytes](https://huggingface.co/docs/bitsandbytes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0fb693",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab0fb693",
        "outputId": "fdd39ca8-47e4-4988-ac07-1ce362ed56a8"
      },
      "outputs": [],
      "source": [
        "model_8bit = AutoModelForMaskedLM.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    load_in_8bit=True,    # activates 8-bit quantization\n",
        "    device_map=\"auto\"     # automatically places model on available device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4324df1d",
      "metadata": {
        "id": "4324df1d"
      },
      "source": [
        "**`TODO:`** Load both the full-precision and 8-bit quantized versions of 'bert-base-uncased', then compute and print each model‚Äôs disk size (from the HF cache folder) and in-memory / VRAM usage side-by-side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e94b988",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e94b988",
        "outputId": "f4bcf605-3120-4bb8-9f55-f6ee9bd78362"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "56635aa2",
      "metadata": {
        "id": "56635aa2"
      },
      "source": [
        "**`TODO:`** Load both the full and quantized models in MLMs pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b43c58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76b43c58",
        "outputId": "d2a21c0c-bb41-4994-97e2-ff29845232b2"
      },
      "outputs": [],
      "source": [
        "non_quantized = ...\n",
        "quantized = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d330e95",
      "metadata": {
        "id": "8d330e95"
      },
      "source": [
        "### ‚öñÔ∏è Task: Compare Quantized vs. Full-Precision Models\n",
        "\n",
        "Now you‚Äôll evaluate how **quantization** affects the performance of a model.  \n",
        "You‚Äôll compare the original full-precision **BERT** model with its **quantized (8-bit)** version loaded using BitsAndBytes.\n",
        "\n",
        "In this section, you‚Äôll write a loop that runs both models on the same masked sentences and records several key metrics:\n",
        "\n",
        "1. **Inference time**  \n",
        "   Measure how long each model takes to generate predictions.  \n",
        "   This will show how much faster the quantized model runs compared to the full-precision version.\n",
        "\n",
        "2. **Top-1 agreement**  \n",
        "   Check whether both versions predict the **same top word** for the `[MASK]` position.\n",
        "\n",
        "3. **Top-5 overlap**  \n",
        "   Compare the sets of their top-5 predictions. Count how many of the full-precision model‚Äôs top-5 words also appear in the quantized model‚Äôs top-5 list.\n",
        "\n",
        "4. **KL divergence**  \n",
        "   Compute the Kullback‚ÄìLeibler divergence between the two models‚Äô predicted probability distributions.  \n",
        "   A lower KL value means the quantized model‚Äôs predictions are closer to the original.\n",
        "\n",
        "After evaluating all samples, calculate and print the **average results** across the dataset.  \n",
        "These metrics will help you see how much **accuracy is retained** and how much **speed is gained** through quantization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aae1f0c",
      "metadata": {
        "id": "6aae1f0c"
      },
      "source": [
        "**`TODO:`** Implement the experiment as described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d173752",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d173752",
        "outputId": "087c0499-e149-4206-d0fb-e4243d8819d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f17bebc8",
      "metadata": {
        "id": "f17bebc8"
      },
      "source": [
        "**`TODO:`** Plot the Top-1 Agreement and the Top-5 Overlap between the student and the teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037604fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "037604fb",
        "outputId": "76d779ca-4ecc-4406-95c0-080b7f418881"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
