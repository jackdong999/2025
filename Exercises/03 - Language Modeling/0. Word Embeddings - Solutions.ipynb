{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7a0e9d",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561fcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from typing import Any\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08048d5c",
   "metadata": {},
   "source": [
    "## Exercise 1: Training a Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97858931",
   "metadata": {},
   "source": [
    "### Preparing the Corpus and Stopwords\n",
    "\n",
    "To train our own **Word2Vec embeddings**, we first need a text corpus.  \n",
    "Here we‚Äôll use the **Brown Corpus**, a classic collection of English texts available through NLTK.  \n",
    "\n",
    "- We download the `brown` corpus and the list of common English **stopwords**.  \n",
    "- Stopwords (like *the*, *and*, *is*) carry little semantic meaning, so we‚Äôll filter them out before training.  \n",
    "\n",
    "This ensures our Word2Vec model focuses on more informative words when learning embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89825ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/nearchospotamitis/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nearchospotamitis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82edba5",
   "metadata": {},
   "source": [
    "### Exploring the Brown Corpus\n",
    "\n",
    "Before training Word2Vec, let‚Äôs take a look at the data we‚Äôll be working with.  \n",
    "\n",
    "- The **Brown Corpus** is divided into categories (e.g., news, fiction, humor).  \n",
    "- We print the available categories and the total number of sentences in the corpus.  \n",
    "- For illustration, we also display the first few sentences so we can see how the raw data looks.  \n",
    "\n",
    "This step helps us understand the **structure and style of the text** before preprocessing and training embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5872758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Total sentences: 57340\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n"
     ]
    }
   ],
   "source": [
    "# Show some info about the corpus\n",
    "print(\"Categories:\", brown.categories())\n",
    "print(\"Total sentences:\", len(brown.sents()))\n",
    "\n",
    "# Get 100 senteces from the humor category\n",
    "dataset = brown.sents()\n",
    "\n",
    "for i in range(3):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a00a5c",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing the Text\n",
    "\n",
    "Raw sentences from the Brown Corpus need to be **preprocessed** before they can be used for training Word2Vec.  \n",
    "\n",
    "Steps applied:  \n",
    "1. **Join and tokenize** each sentence into words.  \n",
    "2. **Lowercasing & deaccenting** (`deacc=True` removes punctuation/accents).  \n",
    "3. **Filter short words** (`min_len=2` keeps words with at least 2 characters).  \n",
    "4. **Remove stopwords** (like *the*, *and*, *is*) to focus on meaningful content.  \n",
    "\n",
    "The result is a list of cleaned, tokenized sentences.  \n",
    "We print a few examples to see how the preprocessing transforms the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08fbee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57340/57340 [00:03<00:00, 14943.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'atlanta', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "['jury', 'said', 'term', 'end', 'presentments', 'city', 'executive', 'committee', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted']\n",
      "['september', 'october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard', 'fought', 'primary', 'mayor', 'nominate', 'ivan', 'allen', 'jr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_sentences = [\n",
    "    [w for w in simple_preprocess(\" \".join(sent), deacc=True, min_len=2) if w not in stop_words]\n",
    "    for sent in tqdm(dataset)\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    print(cleaned_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae961c5",
   "metadata": {},
   "source": [
    "### Training a Word2Vec Model\n",
    "\n",
    "Now we train our own **Word2Vec embeddings** on the Brown Corpus.  \n",
    "\n",
    "Key parameters:  \n",
    "- `vector_size=50`: each word is represented as a 50-dimensional vector.  \n",
    "- `window=3`: the model looks at 3 words to the left and right for context.  \n",
    "- `min_count=1`: keep all words (even rare ones).  \n",
    "- `sg=1`: use the **skip-gram** approach, which works well for smaller datasets.  \n",
    "- `epochs=20`: make multiple passes over the data to learn stronger embeddings.  \n",
    "\n",
    "After training, we can:  \n",
    "- Inspect the learned vector for a specific word (here, `\"engineer\"`).  \n",
    "- Find its **nearest neighbors** in the embedding space, i.e., words with similar meanings or usage contexts.  \n",
    "\n",
    "This demonstrates how Word2Vec captures **semantic similarity** directly from the text we trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09af82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'jury':\n",
      "[ 0.24724108  0.56967384 -0.26902926 -0.04490588 -0.10816167  0.25551325\n",
      "  0.3104594   0.46434903 -0.6406326   0.39196825]\n",
      "\n",
      "Most similar to 'jury':\n",
      "[('wexler', 0.7871246933937073), ('witnesses', 0.7546661496162415), ('subpenas', 0.7486013770103455), ('bail', 0.7438120245933533), ('appoint', 0.7372197508811951)]\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=cleaned_sentences,\n",
    "    vector_size=50,   # embedding size \n",
    "    window=3,         # context window\n",
    "    min_count=1,      # keep all words \n",
    "    workers=4,        # number of CPU cores to use\n",
    "    sg=1,             # skip-gram (better for small data)\n",
    "    epochs=20,        # more passes to learn something\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "word = \"jury\"\n",
    "# Inspect a word vector\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(model.wv[word][:10])   # show first 10 dimensions\n",
    "\n",
    "# Check nearest neighbors\n",
    "print(f\"\\nMost similar to '{word}':\")\n",
    "print(model.wv.most_similar(word, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce606d9",
   "metadata": {},
   "source": [
    "## Exercise 2: Using GloVe Embeddings to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffc42d",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Word Embeddings\n",
    "\n",
    "Instead of learning word vectors from scratch, we can use **pre-trained embeddings** that capture semantic relationships between words.  \n",
    "Here we download the **GloVe embeddings** (Global Vectors for Word Representation) trained on a large Wikipedia + Gigaword corpus.  \n",
    "\n",
    "- Each word is mapped to a **100-dimensional vector**.  \n",
    "- Words that appear in similar contexts (e.g., *king* and *queen*) will have vectors that are close to each other in this space.  \n",
    "\n",
    "These embeddings will serve as the foundation for representing text in our binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d4873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe92640",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "\n",
    "For this example, we‚Äôll use the **Sentiment140 Twitter dataset**, which contains tweets labeled for **sentiment (positive or negative)**.  \n",
    "This is a common benchmark dataset for text classification tasks.  \n",
    "\n",
    "- We load the dataset using the ü§ó **`datasets`** library.  \n",
    "- To keep the demo lightweight and fast, we only take a **subset** of the data:  \n",
    "  - 50,000 tweets for training  \n",
    "  - 10,000 tweets for testing  \n",
    "\n",
    "Finally, we print the dataset sizes to confirm our selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943edfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 50000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"adilbekovich/Sentiment140Twitter\")\n",
    "\n",
    "train = ds[\"train\"].select(range(50_000))\n",
    "test = ds[\"test\"].select(range(10_000))\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b0f4",
   "metadata": {},
   "source": [
    "### Converting Tweets into Embeddings\n",
    "\n",
    "Machine learning models can‚Äôt directly process raw text, so we need to convert each tweet into a **numeric vector**.  \n",
    "\n",
    "We define a function `sentence_embedding` that:  \n",
    "1. **Tokenizes** the sentence into words (removing punctuation and making everything lowercase).  \n",
    "2. Looks up the **GloVe vector** for each word.  \n",
    "3. Computes the **average** of all word vectors to create a single fixed-length representation of the entire sentence.  \n",
    "   - If a sentence has no known words, we assign a zero vector.  \n",
    "\n",
    "Then we:  \n",
    "- Apply this function to every tweet in the train and test sets.  \n",
    "- Store the result in a new column called `\"embeddings\"`.  \n",
    "- Format the dataset so that `\"embeddings\"` and `\"label\"` are ready to be used as **PyTorch tensors** for training a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9a9a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd3a4333c524d1e86234117c9888496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929d458294ee46159746f70edf6d9547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sentence_embedding(sentence, model):\n",
    "    tokens = list(tokenize(sentence, deacc=True, to_lower=True))\n",
    "    word_vectors = [model[w] for w in tokens if w in model]\n",
    "    sentence_vector = np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return sentence_vector\n",
    "    \n",
    "train = train.map(lambda x: {\"embeddings\": sentence_embedding(x['text'], glove_vectors)})\n",
    "test = test.map(lambda x: {\"embeddings\": sentence_embedding(x['text'], glove_vectors)})\n",
    "\n",
    "train.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])\n",
    "test.set_format(type=\"torch\", columns=[\"embeddings\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7999a34",
   "metadata": {},
   "source": [
    "### Building a Simple Neural Network Classifier\n",
    "\n",
    "Now that we have numeric embeddings for each tweet, we can train a **neural network** to classify sentiment.  \n",
    "\n",
    "We define a PyTorch model `SentimentClassifier` with the following structure:  \n",
    "\n",
    "1. **Input layer**: takes in the 100-dimensional GloVe embedding for each tweet.  \n",
    "2. **Hidden layer**: a fully connected layer with 256 units and a **ReLU activation**, which introduces non-linearity.  \n",
    "3. **Output layer**: a single neuron that predicts the probability of the tweet being **positive** (values between 0 and 1).  \n",
    "   - We use a **sigmoid activation** to squash the output.  \n",
    "\n",
    "This simple feed-forward network is powerful enough to learn sentiment patterns from our averaged word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2149cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # final activation for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)         # hidden layer uses ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)      # output between 0‚Äì1\n",
    "        return x\n",
    "\n",
    "model = SentimentClassifier(input_dim=glove_vectors.vector_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c06dd9",
   "metadata": {},
   "source": [
    "### Setting Up Training Parameters\n",
    "\n",
    "Before training the model, we need to define some key components:  \n",
    "\n",
    "- **Batch size (64)**: the number of samples processed at once before updating the model‚Äôs parameters.  \n",
    "- **Epochs (30)**: how many times the model will see the entire training dataset.  \n",
    "- **Loss function**: we use **Binary Cross-Entropy Loss (`BCELoss`)**, which is standard for binary classification tasks.  \n",
    "- **Optimizer**: we use **Stochastic Gradient Descent (SGD)** with a learning rate of 0.01 to update model weights during training.  \n",
    "\n",
    "We also wrap our dataset into **DataLoaders**:  \n",
    "- `train_dataloader`: feeds batches of tweets into the model, shuffling to avoid order bias.  \n",
    "- `test_dataloader`: used for evaluation (no shuffling needed).  \n",
    "\n",
    "This setup prepares us for the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682cf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "loss_fn = nn.BCELoss()\n",
    "acc_fn = BinaryAccuracy()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f2508",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "\n",
    "Now we train our sentiment classifier using the training data.  \n",
    "\n",
    "For each **epoch** (full pass over the training set):  \n",
    "1. **Batching**: the `DataLoader` gives us a batch of tweet embeddings and labels.  \n",
    "2. **Forward pass**: the embeddings are passed through the model to get predictions.  \n",
    "3. **Loss calculation**: compare predictions with the true labels using **binary cross-entropy**.  \n",
    "4. **Backward pass**: compute gradients of the loss with respect to model parameters.  \n",
    "5. **Optimizer step**: update model weights using **SGD**.  \n",
    "6. **Repeat** for all batches in the epoch.  \n",
    "\n",
    "At the end of each epoch, we print the **average training loss**, which shows how well the model is learning over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c49f9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6851749420166016\n",
      "Epoch 2/30, Loss: 0.6702383160591125\n",
      "Epoch 3/30, Loss: 0.6522340774536133\n",
      "Epoch 4/30, Loss: 0.6332014799118042\n",
      "Epoch 5/30, Loss: 0.6167123913764954\n",
      "Epoch 6/30, Loss: 0.6037730574607849\n",
      "Epoch 7/30, Loss: 0.5947628021240234\n",
      "Epoch 8/30, Loss: 0.5883857011795044\n",
      "Epoch 9/30, Loss: 0.5839075446128845\n",
      "Epoch 10/30, Loss: 0.5803468227386475\n",
      "Epoch 11/30, Loss: 0.5778124332427979\n",
      "Epoch 12/30, Loss: 0.5757883787155151\n",
      "Epoch 13/30, Loss: 0.5743950009346008\n",
      "Epoch 14/30, Loss: 0.5728640556335449\n",
      "Epoch 15/30, Loss: 0.5719155073165894\n",
      "Epoch 16/30, Loss: 0.5706758499145508\n",
      "Epoch 17/30, Loss: 0.5699134469032288\n",
      "Epoch 18/30, Loss: 0.5693798065185547\n",
      "Epoch 19/30, Loss: 0.5684167146682739\n",
      "Epoch 20/30, Loss: 0.5680344700813293\n",
      "Epoch 21/30, Loss: 0.5674179196357727\n",
      "Epoch 22/30, Loss: 0.5669409036636353\n",
      "Epoch 23/30, Loss: 0.5665177702903748\n",
      "Epoch 24/30, Loss: 0.5659852027893066\n",
      "Epoch 25/30, Loss: 0.5651564598083496\n",
      "Epoch 26/30, Loss: 0.5645962953567505\n",
      "Epoch 27/30, Loss: 0.5641855597496033\n",
      "Epoch 28/30, Loss: 0.5637066960334778\n",
      "Epoch 29/30, Loss: 0.5637319684028625\n",
      "Epoch 30/30, Loss: 0.5630297660827637\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i,  batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Get the inputs and labels from the batch\n",
    "        inputs = batch['embeddings']\n",
    "        labels = batch['label']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(outputs.squeeze().float(), labels.squeeze().float())\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9bbef",
   "metadata": {},
   "source": [
    "### Evaluating the Model on Test Data\n",
    "\n",
    "After training, we switch the model to **evaluation mode** (`model.eval()`) to test its performance.  \n",
    "\n",
    "For each batch in the test set:  \n",
    "1. **Forward pass**: compute predictions for the embeddings.  \n",
    "2. **Loss calculation**: measure how far predictions are from the true labels.  \n",
    "3. **Thresholding**: since outputs are probabilities (0‚Äì1), we assign  \n",
    "   - `1` if prediction > 0.5 (positive sentiment)  \n",
    "   - `0` otherwise (negative sentiment).  \n",
    "4. **Accuracy**: compare predictions with labels to compute the percentage of correct classifications.  \n",
    "\n",
    "Finally, we average the results across all batches and print the **test accuracy**, which tells us how well the model generalizes to unseen tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a26fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 70.97%\n"
     ]
    }
   ],
   "source": [
    "batch_loss = 0\n",
    "batch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "for i,  batch in enumerate(test_dataloader):\n",
    "    # get the inputs\n",
    "    inputs = batch['embeddings']\n",
    "    labels = batch['label']\n",
    "    \n",
    "    # run forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    test_loss = loss_fn(outputs.squeeze().float(), labels.squeeze().float())\n",
    "    predictions = torch.tensor([1. if i > 0.5 else 0. for i in outputs])\n",
    "    \n",
    "    \n",
    "    acc = acc_fn(labels, predictions)\n",
    "    \n",
    "    batch_loss += test_loss.item()\n",
    "    batch_acc += acc.item()\n",
    "\n",
    "test_loss = batch_loss / len(test_dataloader)\n",
    "test_acc = batch_acc / len(test_dataloader)\n",
    "\n",
    "print(f'Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a77c66",
   "metadata": {},
   "source": [
    "## 3. MCQ\n",
    "\n",
    "### 3.1. Purpose of Word Embeddings\n",
    "\n",
    "What is the main purpose of word embeddings in NLP?\n",
    "\n",
    "A. To convert words into high-dimensional one-hot vectors <br>\n",
    "B. To map words into continuous vector spaces that capture semantic meaning<br>\n",
    "C. To remove stopwords from text before processing<br>\n",
    "D. To reduce the training time of convolutional networks<br>\n",
    "\n",
    "**Answer:** B ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. One-Hot vs. Embeddings\n",
    "\n",
    "Compared to one-hot encoding, word embeddings:\n",
    "\n",
    "A. Have the same dimensionality as the vocabulary size<br>\n",
    "B. Provide dense, low-dimensional representations that capture similarities<br>\n",
    "C. Are always manually designed by experts<br>\n",
    "D. Cannot be trained with neural networks<br>\n",
    "\n",
    "**Answer:** B ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3. Word2Vec Models\n",
    "\n",
    "The Skip-gram model in Word2Vec is designed to:\n",
    "\n",
    "A. Predict the context words given a target word<br>\n",
    "B. Predict the target word given the context words<br>\n",
    "C. Cluster words into fixed categories<br>\n",
    "D. Remove rare words from the corpus<br>\n",
    "\n",
    "**Answer:** A ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4. Embedding Matrix Shape\n",
    "\n",
    "In a neural network with vocabulary size $V$ and embedding dimension $d$, the embedding matrix has shape:\n",
    "\n",
    "A. $(d \\times V)$<br>\n",
    "B. $(V \\times d)$<br>\n",
    "C. $(V \\times V)$<br>\n",
    "D. $(d \\times d)$<br>\n",
    "\n",
    "**Answer:** B ‚úÖ (rows = words, columns = embedding dimensions)\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5. Semantic Relationships\n",
    "\n",
    "Word embeddings can capture analogies such as:\n",
    "\n",
    "A. king ‚Äì man + woman ‚âà queen<br>\n",
    "B. dog ‚Äì cat + car ‚âà airplane<br>\n",
    "C. apple ‚Äì red + fast ‚âà running<br>\n",
    "D. chair ‚Äì table + sky ‚âà cloud<br>\n",
    "\n",
    "**Answer:** A ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.6. Contextual vs. Static Embeddings\n",
    "\n",
    "How do contextual embeddings (e.g., BERT) differ from static embeddings (e.g., Word2Vec)?\n",
    "\n",
    "A. They assign the same vector to a word regardless of context<br>\n",
    "B. They assign different vectors to a word depending on its context<br>\n",
    "C. They are always lower-dimensional than static embeddings<br>\n",
    "D. They do not require pretraining on large corpora<br>\n",
    "\n",
    "**Answer:** B ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.7. Sparse vs. Dense Representations\n",
    "\n",
    "Compared to Bag-of-Words (BoW) vectors, neural word embeddings are:\n",
    "\n",
    "A. Higher dimensional and sparse<br>\n",
    "B. Always binary representations<br>\n",
    "C. Lower dimensional and sparse<br>\n",
    "D. Lower dimensional and dense<br>\n",
    "\n",
    "**Answer:** D ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.8. Semantic Similarity\n",
    "\n",
    "What is the main advantage of word embeddings over one-hot encodings?\n",
    "\n",
    "A. They guarantee perfect accuracy in classification tasks<br>\n",
    "B. They eliminate the need for training neural networks<br>\n",
    "C. They capture semantic similarity between words in vector space<br>\n",
    "D. They automatically remove stopwords from text<br>\n",
    "\n",
    "**Answer:** C ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.9. CBOW vs. Skip-gram\n",
    "\n",
    "The Continuous Bag of Words (CBOW) model aims to:\n",
    "\n",
    "A. Predict the target word given its surrounding context words<br>\n",
    "B. Assign unique one-hot vectors to words<br>\n",
    "C. Predict the context words given a target word<br>\n",
    "D. Cluster words into topics using SVD<br>\n",
    "\n",
    "**Answer:** A ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 3.10. Distributional Semantics\n",
    "\n",
    "The idea that ‚Äúyou shall know a word by the company it keeps‚Äù refers to:\n",
    "\n",
    "A. Overfitting in NLP models<br>\n",
    "B. Context-based learning of embeddings<br>\n",
    "C. Sentence segmentation<br>\n",
    "D. Stopword removal<br>\n",
    "\n",
    "**Answer:** B ‚úÖ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
