{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931b3d61",
   "metadata": {},
   "source": [
    "# ðŸ§  Building Decoding Strategies from Scratch\n",
    "\n",
    "### ðŸŽ¯ Objective\n",
    "In this session, weâ€™ll **recreate common decoding strategies step by step**, using only the modelâ€™s raw output probabilities (logits).  \n",
    "Youâ€™ve already seen how Hugging Faceâ€™s `generate()` method can perform greedy search, beam search, top-k, and nucleus (top-p) sampling for you.  \n",
    "Now itâ€™s time to **open the black box** and see *how those algorithms actually work under the hood.*\n",
    "\n",
    "\n",
    "### ðŸ§© What Youâ€™ll Learn\n",
    "By the end of this notebook, youâ€™ll be able to:\n",
    "- Access token probabilities from a language model (e.g., GPT-2).  \n",
    "- Implement your own:\n",
    "  - **Greedy Search**\n",
    "  - **Beam Search**\n",
    "  - **Top-k Sampling**\n",
    "  - **Nucleus (Top-p) Sampling**\n",
    "- Compare their behavior and outputs across different prompts.  \n",
    "- Understand the trade-offs between determinism, diversity, and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd65980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f7678",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup and First Generation\n",
    "\n",
    "Before we start building decoding strategies manually, letâ€™s load a pretrained language model and generate some text using the **built-in Hugging Face API**.  \n",
    "This will serve as our **baseline** â€” weâ€™ll soon replace this simple `.generate()` call with our own decoding logic.\n",
    "\n",
    "\n",
    "`**TODO:**`\n",
    "\n",
    "1. **Import dependencies** â€” weâ€™ll use `transformers` for the model and tokenizer, and `torch` for tensor operations.  \n",
    "2. **Load GPT-2** â€” a small, autoregressive model trained to predict the next token in a sequence.  \n",
    "3. **Prepare the input** â€” weâ€™ll encode a short prompt into token IDs.  \n",
    "4. **Generate text** â€” using the modelâ€™s default decoding (greedy search).  \n",
    "5. **Decode the output** â€” back to human-readable text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397cd945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of being a doctor.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text = \"I have a dream\"\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66f1a6",
   "metadata": {},
   "source": [
    "## ðŸš€ Implementing Greedy Search\n",
    "\n",
    "### ðŸ§  Concept\n",
    "\n",
    "Greedy Search is the most straightforward decoding method:\n",
    "- At each step, the model predicts a probability distribution over the vocabulary.\n",
    "- We pick **only the most probable token** (the one with the highest logit or probability).\n",
    "- That token is appended to the sequence and fed back into the model.\n",
    "- The process repeats until we reach the desired length or an end-of-sentence token.\n",
    "\n",
    "This strategy is **deterministic** and **fast**, but often leads to repetitive or locally optimal results â€” the model never explores alternative continuations.\n",
    "\n",
    "### âš™ï¸ Implementation Hints\n",
    "\n",
    "In this method:\n",
    "1. We run the model on the current sequence to obtain the logits.\n",
    "2. We take the **argmax** over the last-token logits to choose the next token.\n",
    "3. We append that token to the input sequence.\n",
    "4. We recursively continue until weâ€™ve generated the target number of tokens.\n",
    "\n",
    "**`TODO:`** Implement the `greedy_search` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a89115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of being a doctor.\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int = 5            # Number of tokens left to generate\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Greedy Search decoding.\n",
    "\n",
    "    At each step, the model selects the most probable next token\n",
    "    (the one with the highest logit value) and appends it to the sequence.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "    token_id = torch.argmax(logits).unsqueeze(0)\n",
    "\n",
    "    # Add the predicted token to the list of input ids\n",
    "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "    # Recursive call\n",
    "    input_ids = greedy_search(new_input_ids, length-1)\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Start generating text\n",
    "output_ids = greedy_search(input_ids, length=5)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81363d17",
   "metadata": {},
   "source": [
    "## ðŸ§® Helper Function: `get_log_prob`\n",
    "\n",
    "This small utility computes the **log-probability** of a chosen token given the modelâ€™s output logits.  \n",
    "Itâ€™s useful for tracking and comparing sequence scores across decoding strategies.\n",
    "\n",
    "**How it works:**\n",
    "1. Applies a softmax to convert logits into probabilities.  \n",
    "2. Takes the logarithm of those probabilities.  \n",
    "3. Returns the log-probability corresponding to the selected `token_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aaa7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(logits, token_id):\n",
    "    # Compute the softmax of the logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    log_probabilities = torch.log(probabilities)\n",
    "    \n",
    "    # Get the log probability of the token\n",
    "    token_log_probability = log_probabilities[token_id].item()\n",
    "\n",
    "    return token_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a85571",
   "metadata": {},
   "source": [
    "## ðŸš€ Implementing Beam Search\n",
    "\n",
    "### ðŸ§  Concept\n",
    "\n",
    "Beam Search improves upon Greedy Search by keeping track of **multiple candidate sequences** (called *beams*) instead of just one.  \n",
    "At each generation step, instead of picking only the single most likely token, we explore the **top-k next tokens** for each current sequence (where *k* is the beam width).  \n",
    "We then keep only the *k* best overall sequences based on their **cumulative log-probability scores**.\n",
    "\n",
    "This strategy balances **exploration** and **exploitation** â€” it often produces more coherent text than Greedy Search, though itâ€™s more computationally expensive.\n",
    "\n",
    "### âš™ï¸ Implementation Hints\n",
    "\n",
    "In this implementation:\n",
    "1. For each call, we compute the modelâ€™s output logits for the current sequence.  \n",
    "2. We extract the **top-k tokens** (beam width) with the highest logit values.  \n",
    "3. For each of these tokens:\n",
    "   - Compute its log-probability.  \n",
    "   - Append it to the input sequence.  \n",
    "   - Recursively continue the search for the remaining steps.  \n",
    "4. Once all beams reach the target length, we return all completed sequences with their total scores and pick the **best one**.\n",
    "\n",
    "**`TODO:`** Implement the `beam_search` function below.  \n",
    "Focus on:\n",
    "- Expanding each beam with the top-k tokens.  \n",
    "- Keeping track of cumulative scores.  \n",
    "- Returning all completed sequences so you can later select the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0866f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Generated text: I have a dream. I have a dream\n"
     ]
    }
   ],
   "source": [
    "def beam_search(\n",
    "    input_ids: torch.Tensor,       # Current input token IDs (shape: [1, seq_len])\n",
    "    length: int,                   # Number of tokens left to generate\n",
    "    beams: int,                    # Beam width (number of candidate sequences to keep)\n",
    "    score: Optional[float] = None  # Cumulative log-probability of the sequence so far\n",
    ") -> List[Dict[str, torch.Tensor]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs recursive Beam Search decoding.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        beams: Number of top candidate tokens to expand at each step.\n",
    "        score: Optional cumulative log-probability for the sequence so far.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing:\n",
    "            - \"new_input_ids\": the generated token sequence (tensor)\n",
    "            - \"score\": the cumulative log-probability score (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return [{\"new_input_ids\": input_ids, \"score\": score}]\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Select the top-k most probable next tokens (beam expansion)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    top_token_ids = torch.topk(logits, beams).indices\n",
    "\n",
    "    outputs = []\n",
    "    for j, token_id in enumerate(top_token_ids):\n",
    "\n",
    "        # Compute the score of the predicted token\n",
    "        token_score = get_log_prob(logits, token_id)\n",
    "        cumulative_score = token_score + (score if score is not None else 0.0)\n",
    "\n",
    "        # Add the predicted token to the list of input ids\n",
    "        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "        # Recursive call\n",
    "        outs = beam_search(new_input_ids, length-1, beams, cumulative_score)\n",
    "        outputs.extend(outs)\n",
    "    return outputs\n",
    "\n",
    "# Start generating text\n",
    "output_ids = beam_search(input_ids, length=5, beams=2)\n",
    "best_entry = max(output_ids, key=lambda x: x[\"score\"])\n",
    "best_output = tokenizer.decode(best_entry[\"new_input_ids\"].squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Best Generated text: {best_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a19f1",
   "metadata": {},
   "source": [
    "## ðŸš€ Implementing Top-k Sampling from Scratch\n",
    "\n",
    "### ðŸ§  Concept\n",
    "\n",
    "So far, **Greedy Search** and **Beam Search** always choose the most likely tokens â€” making them deterministic but sometimes repetitive or predictable.  \n",
    "To introduce more **creativity and diversity**, we can use **sampling-based decoding** methods.\n",
    "\n",
    "**Top-k Sampling** limits randomness to a controlled subset of possible next tokens:\n",
    "1. At each step, we look at the modelâ€™s logits for all tokens.\n",
    "2. We keep only the **top-k most probable tokens**.\n",
    "3. We **mask out** all other tokens by setting their logits to `-inf`.\n",
    "4. We apply a softmax over the remaining tokens to obtain a proper probability distribution.\n",
    "5. We **randomly sample** one token from this smaller set.\n",
    "6. Append the chosen token and continue.\n",
    "\n",
    "This way, we donâ€™t always pick the top token (like in greedy search), but we avoid sampling from the long tail of improbable words â€” a balance between **coherence** and **diversity**.\n",
    "\n",
    "### âš™ï¸ Implementation Hints\n",
    "\n",
    "In this method:\n",
    "- Use `torch.topk(logits, top_k)` to find the cutoff threshold.  \n",
    "- Set all logits below that threshold to `-inf` (so their probability becomes zero after softmax).  \n",
    "- Use `torch.multinomial()` to sample one token ID according to the resulting probability distribution.  \n",
    "- Recursively repeat the process for the desired number of tokens.  \n",
    "- Remember to use a **manual seed** (e.g., `torch.manual_seed(0)`) for reproducibility in experiments.\n",
    "\n",
    "\n",
    "**`TODO:`** Implement the `top_k_sampling` function below.  \n",
    "Try different `top_k` values (e.g., 5, 20, 100) and observe how the outputâ€™s **creativity** changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22c0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream,\" she said, referring\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int                 # Number of top tokens to sample from at each step\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling decoding.\n",
    "\n",
    "    At each step:\n",
    "    1. We take the modelâ€™s logits for the next token.\n",
    "    2. We keep only the top-k most probable tokens.\n",
    "    3. We apply softmax to get a probability distribution.\n",
    "    4. We sample one token from that reduced set (introducing controlled randomness).\n",
    "    5. Append it to the sequence and continue recursively.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    assert top_k >= 1\n",
    "\n",
    "    # Setting the manual seed for reproducibility. This is done in a simplistic way here for demonstration purposes.\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    new_logits = torch.clone(logits)\n",
    "    new_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(new_logits, dim=-1)\n",
    "\n",
    "    # Sample n tokens from the resulting distribution\n",
    "    top_token_id = torch.multinomial(probabilities, 1)[0]\n",
    "    \n",
    "    new_input_ids = torch.cat([input_ids, top_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    out = top_k_sampling(new_input_ids, length-1, top_k)\n",
    "    return out\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4a495",
   "metadata": {},
   "source": [
    "## ðŸŒ¡ï¸ Adding Temperature to Top-k Sampling\n",
    "\n",
    "### ðŸ§  Concept\n",
    "**Temperature** controls how â€œconfidentâ€ or â€œcreativeâ€ the modelâ€™s sampling behavior is.  \n",
    "Before applying softmax, we divide the logits by a temperature value:\n",
    "\n",
    "\\begin{align}\n",
    "p_i = \\text{softmax}\\left(\\frac{\\text{logits}_i}{T}\\right)\n",
    "\\end{align}\n",
    "\n",
    "- **Low temperature (< 1)** â†’ sharper distribution â†’ more deterministic, focused outputs  \n",
    "- **High temperature (> 1)** â†’ flatter distribution â†’ more random, diverse outputs  \n",
    "\n",
    "**`TODO:`**  \n",
    "Modify the Top-k Sampling implementation so that logits are divided by `temperature` **before** applying softmax.  \n",
    "Then, experiment with different temperature values (e.g., `0.7`, `1.0`, `1.5`) and observe how the outputâ€™s creativity changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c46297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: I have a dream of becoming one,\" she\n"
     ]
    }
   ],
   "source": [
    "def top_k_sampling(\n",
    "    input_ids: torch.Tensor,   # Current sequence of token IDs (shape: [1, seq_len])\n",
    "    length: int,               # Number of tokens left to generate\n",
    "    top_k: int,                # Number of top tokens to sample from at each step\n",
    "    temperature: float = 1.0   # Temperature parameter controlling randomness\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs recursive Top-k Sampling with Temperature.\n",
    "\n",
    "    This decoding method introduces *controlled randomness*:\n",
    "    - Restricts sampling to the top-k most probable tokens.\n",
    "    - Scales the logits by a temperature before applying softmax.\n",
    "\n",
    "    Args:\n",
    "        input_ids: Current sequence of token IDs (1 x seq_len tensor).\n",
    "        length: Number of tokens left to generate.\n",
    "        top_k: Number of highest-probability tokens to consider at each step.\n",
    "        temperature: Value > 0 that controls randomness.\n",
    "            â€¢ Lower (<1) â†’ sharper distribution, more deterministic.\n",
    "            â€¢ Higher (>1) â†’ flatter distribution, more random.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the full generated token sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if length == 0:\n",
    "        return input_ids\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # Get the predicted next sub-word (here we use top-k search)\n",
    "    logits = predictions[0, -1, :]\n",
    "\n",
    "    assert top_k >= 1\n",
    "\n",
    "    # Setting the manual seed for reproducibility. This is done in a simplistic way here for demonstration purposes.\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    new_logits = torch.clone(logits)\n",
    "    new_logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
    "\n",
    "    # Sample n tokens from the resulting distribution\n",
    "    top_token_id = torch.multinomial(probabilities, 1)[0]\n",
    "\n",
    "    \n",
    "    token_score = get_log_prob(logits, top_token_id)\n",
    "    new_input_ids = torch.cat([input_ids, top_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "\n",
    "    out = top_k_sampling(new_input_ids, length-1, top_k, 1)\n",
    "    return out\n",
    "\n",
    "output_ids = top_k_sampling(input_ids, length=5, top_k=20, temperature=0.2)\n",
    "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "print(f\"Generated text: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_private",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
